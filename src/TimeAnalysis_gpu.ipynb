{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a064639f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#%matplotlib notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04c48500",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import functools\n",
    "import hashlib\n",
    "import itertools\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "from functools import partial, reduce\n",
    "from glob import glob\n",
    "from multiprocessing import Pool\n",
    "from time import sleep\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import portalocker as pl\n",
    "import seaborn as sns\n",
    "from matplotlib import cm\n",
    "from tqdm import tqdm\n",
    "from matplotlib.lines import Line2D\n",
    "from copy import deepcopy\n",
    "from plot_metrics import preprocess_metrics\n",
    "from matplotlib.ticker import LogLocator\n",
    "import matplotlib.patches as mpatches\n",
    "from util.metrics_utils import *\n",
    "from util.colour_mapper import colour_mapper\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "87eeb204",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_results(_df_tr, df_tr, _df_ti):    \n",
    "    appr_list = ['joint', 'backbonefreezing' ]\n",
    "    t = _df_tr[~((_df_tr['Approach'].isin(appr_list)) & (_df_tr['Memory Size'] == 0))]\n",
    "    _df_tr = deepcopy(t)\n",
    "\n",
    "    t = df_tr[~((df_tr['Approach'].isin(appr_list)) & (df_tr['Memory Size'] == 0))]\n",
    "    df_tr = deepcopy(t)    \n",
    "    \n",
    "    t = _df_ti[~((_df_ti['Approach'].isin(appr_list)) & (_df_ti['Memory Size'] == 0))]\n",
    "    _df_ti = deepcopy(t) \n",
    "\n",
    "    t = df_tr[~((df_tr['Approach']=='scratch') & (df_tr['Memory Size'] !=0))]\n",
    "    df_tr = deepcopy(t)\n",
    "\n",
    "    t = df_tr[~((df_tr['Approach']=='bic') & (df_tr['Memory Size'] != 1000))]\n",
    "    df_tr = deepcopy(t)\n",
    "    \n",
    "    return _df_tr, df_tr, _df_ti\n",
    "\n",
    "\n",
    "#correzione eeil train\n",
    "def eeil_correction(_df_ti, group):\n",
    "    _df_tr_eeil = _df_tr[(_df_tr['Approach']=='eeil') & (_df_tr['Task']==1)]\n",
    "    temp = _df_tr_eeil.groupby(group)['Epoch'].max().reset_index()\n",
    "    temp_time = _df_tr_eeil.groupby([group])[[group,'Train Time']].head(3).reset_index()\n",
    "    temp_time = temp_time.groupby(group).mean().reset_index()\n",
    "    max_epochs_eeil = max_epochs[max_epochs['Approach']=='eeil']\n",
    "    temp = pd.merge(max_epochs_eeil, temp_time, on=[group])\n",
    "    temp['Time'] = temp[['Epoch', 'Train Time_y']].apply(lambda x: x[0] * x[1], axis=1)\n",
    "    for c in temp[group].unique():\n",
    "        _df_ti.loc[_df_ti[(_df_ti['Approach']=='eeil') & (_df_ti['Type']=='train') & (_df_ti['Task']==1) & (_df_ti[group]==c)].index.values[0], 'Time'] = temp[temp[group]==c]['Time'].values[0]\n",
    "\n",
    "    return _df_ti\n",
    "\n",
    "#correzione eeil train mincpu\n",
    "def eeil_correction_mincpu(_df_ti, group):\n",
    "    _df_tr_eeil = _df_tr[(_df_tr['Approach']=='eeil') & (_df_tr['Task']==1)]\n",
    "    temp = _df_tr_eeil.groupby(group)['Epoch'].max().reset_index()\n",
    "    temp_time = _df_tr_eeil.groupby([group])[[group,'Train Time']].head(3).reset_index()\n",
    "    temp_time = temp_time.groupby(group).min().reset_index()\n",
    "    max_epochs_eeil = max_epochs[max_epochs['Approach']=='eeil']\n",
    "    temp = pd.merge(max_epochs_eeil, temp_time, on=[group])\n",
    "    temp['Time'] = temp[['Epoch', 'Train Time_y']].apply(lambda x: x[0] * x[1], axis=1)\n",
    "    for c in temp[group].unique():\n",
    "        _df_ti.loc[_df_ti[(_df_ti['Approach']=='eeil') & (_df_ti['Type']=='train') & (_df_ti['Task']==1) & (_df_ti[group]==c)].index.values[0], 'Time'] = temp[temp[group]==c]['Time'].values[0]\n",
    "\n",
    "    return _df_ti\n",
    "\n",
    "#correzione eeil train mingpu\n",
    "def eeil_correction_mingpu(_df_ti, group):\n",
    "    _df_tr_eeil = df_tr[(df_tr['Approach']=='eeil') & (df_tr['Task']==1)]\n",
    "    temp = _df_tr_eeil.groupby(group)['Epoch'].max().reset_index()\n",
    "    temp_time = _df_tr_eeil.drop(_df_tr_eeil.groupby([group]).tail(40).index, axis=0)\n",
    "    temp_time = temp_time.groupby(group).min().reset_index()\n",
    "    max_epochs_eeil = max_epochs[max_epochs['Approach']=='eeil']\n",
    "    temp = pd.merge(max_epochs_eeil, temp_time, on=[group])\n",
    "    temp['Time'] = temp[['Epoch_x', 'Train Time_y']].apply(lambda x: x[0] * x[1], axis=1)\n",
    "    for c in temp[group].unique():\n",
    "        _df_ti.loc[_df_ti[(_df_ti['Approach']=='eeil') & (_df_ti['Type']=='train') & (_df_ti['Task']==1) & (_df_ti[group]==c)].index.values[0], 'Time'] = temp[temp[group]==c]['Time'].values[0]\n",
    "    return _df_ti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "8bf424b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def divided_barplot_settings(ax1,ax2,ymax):\n",
    "    d = .015  \n",
    "    kwargs = dict(transform=ax1.transAxes, color='k', clip_on=False)\n",
    "    ax1.plot((-d, +d), (-d, +d), **kwargs)        \n",
    "    ax1.plot((1 - d, 1 + d), (-d, +d), **kwargs)  \n",
    "    kwargs.update(transform=ax2.transAxes)  \n",
    "    ax2.plot((-d, +d), (1 - d, 1 + d), **kwargs)  \n",
    "    ax2.plot((1 - d, 1 + d), (1 - d, 1 + d), **kwargs) \n",
    "    ax1.set_xlabel('')\n",
    "    ax2.set_xlabel('')\n",
    "    ax1.spines['bottom'].set_visible(False)\n",
    "    ax2.spines['top'].set_visible(False)\n",
    "    ax1.set(ylabel=None)\n",
    "    ax2.set(ylabel=None)  \n",
    "    ax2.xaxis.tick_bottom() \n",
    "    ax1.tick_params(bottom=False)  # don't put tick labels at the top\n",
    "    ax2.tick_params(labeltop=False)  # don't put tick labels at the top\n",
    "\n",
    "def add_vertical_lines(ax, ymax, ord_list):\n",
    "    for i,b in enumerate(ax.patches):\n",
    "        if ord_list[i]=='_': \n",
    "            x,y = b.get_xy()\n",
    "            plt.vlines(x = x+b.get_width()/2, ymin = 0, ymax = ymax, color = 'gray', \n",
    "                       linewidth=1.5, linestyle='dashed')\n",
    "            \n",
    "def add_vertical_lines_divided(ax1,ax2,ymax,ord_list):\n",
    "    for i,b in enumerate(ax1.patches):\n",
    "        if ord_list[i]=='_':\n",
    "            x,y = b.get_xy()\n",
    "            x2,y2 = ax2.patches[i].get_xy()\n",
    "            ax1.vlines(x = x+b.get_width()/2, ymin = 0, ymax = ymax, color = 'gray', \n",
    "                           linewidth=1.5, linestyle='dashed')\n",
    "            ax2.vlines(x = x2+b.get_width()/2, ymin = 0, ymax = ymax, color = 'gray', \n",
    "                           linewidth=1.5, linestyle='dashed')\n",
    "\n",
    "        \n",
    "def add_vertical_lines_hatches_colors_divided(ax1,ax2,ymax,ord_list,palette):\n",
    "    '''\n",
    "    for i,b in enumerate(ax1.patches[:14]):\n",
    "        if ord_list[i]=='_': \n",
    "            x,y = b.get_xy()\n",
    "            x2,y2 = ax2.patches[i].get_xy()\n",
    "            ax1.vlines(x = x+b.get_width()/2, ymin = 0, ymax = ymax, color = 'gray', linewidth=1.5)\n",
    "            ax2.vlines(x = x2+b.get_width()/2, ymin = 0, ymax = ymax, color = 'gray', linewidth=1.5)\n",
    "    '''\n",
    "    for i,bar in enumerate (ax1.patches):\n",
    "        bar.set_facecolor(palette[ord_list[i%12]])\n",
    "        ax2.patches[i].set_facecolor(palette[ord_list[i%12]])\n",
    "        if i>=24:\n",
    "            bar.set(hatch = '////')\n",
    "            ax2.patches[i].set(hatch = '////')            \n",
    "\n",
    "def add_vertical_lines_hatches_colors(ax, ymax,ord_list, palette):\n",
    "    for i,b in enumerate(ax.patches[:len(ord_list)]):\n",
    "        if ord_list[i]=='_': \n",
    "            x,y = b.get_xy()\n",
    "            ax.vlines(x = x+b.get_width()/2, ymin = 0, ymax = ymax, color = 'gray', \n",
    "                    linewidth=1.5)\n",
    "    for i,bar in enumerate (ax.patches):                \n",
    "        bar.set_facecolor(palette[ord_list[i%12]])\n",
    "        if i>=24:\n",
    "            bar.set(hatch = '////')\n",
    "            \n",
    "def plot_train_time(df_ti, type, appr_task, group, nclasses, normalize, figsize=None):\n",
    "    ord_list = ['lwfgkd','_','jointmem', 'backbonefreezingmem', 'icarlp', 'lucir', 'ssil', '_','bic', 'eeil', 'ewc','icarl', 'il2m','chen2021']\n",
    "    ord_list = ['lwfgkd','jointmem', 'backbonefreezingmem', 'icarlp', 'lucir', 'ssil','bic', 'eeil', 'ewc','icarl', 'il2m','chen2021']\n",
    "    ord_list = ['bic', 'eeil', 'ewc', 'jointmem', 'backbonefreezingmem', 'icarl', 'icarlp', 'il2m', 'lucir','lwfgkd', 'chen2021', 'ssil']\n",
    "    palette = dict([(k, colour_mapper()[k]) for k in ord_list])\n",
    "    labels = [appr_dict[item] if item!='_' else ' ' for item in ord_list]\n",
    "    labels = [appr_dict[item] if item!='lwfgkd' else 'LWF' for item in ord_list]\n",
    "    if type=='all':\n",
    "        sns.set_style(\"whitegrid\")\n",
    "        df = df_ti[(df_ti['Task']==appr_task) & (df_ti['Type']!='test')].groupby(['Approach',group])['Time'].sum().reset_index()\n",
    "        if normalize: df['Time'] = df['Time'].div(nclasses)\n",
    "        f, (ax1, ax2) = plt.subplots(2, 1, figsize=figsize, sharex=True)\n",
    "        bars1 = sns.barplot(x='Approach', y='Time' , data=df, ax=ax1, order=ord_list, palette=palette)#, ci='sd')\n",
    "        bars2 = sns.barplot(x='Approach', y='Time' , data=df, ax=ax2, order=ord_list, palette=palette)#, ci='sd')\n",
    "        bars1.legend(ax1.patches, [appr_dict[i] for i in ord_list], bbox_to_anchor=(1.05, 1),\n",
    "                     loc=2, borderaxespad=0.1, ncol=1, fontsize=12, frameon=False)\n",
    "        ax2.set_ylim(0, 450)\n",
    "        ax1.set_ylim(1000, 3800)\n",
    "        if not normalize:\n",
    "            ax2.set_ylim(0*nclasses, 450*nclasses)  \n",
    "            ax1.set_ylim(1000*nclasses, 3800*nclasses)\n",
    "        ax2.set_ylabel('Time [s]')\n",
    "        ax2.yaxis.set_label_coords(-0.15,1.02)\n",
    "        ymax = 3800 if normalize else 3800*nclasses\n",
    "        add_vertical_lines_divided(ax1,ax2,ymax,ord_list)                    \n",
    "        divided_barplot_settings(ax1,ax2,ymax)\n",
    "        plt.xticks([])\n",
    "        sns.reset_defaults()\n",
    "\n",
    "    elif type=='all_normalized_scratch':\n",
    "        sns.reset_defaults()\n",
    "        df = df_ti[(df_ti['Task']==appr_task) & (df_ti['Type']!='test')].groupby(['Approach',group])['Time'].sum().reset_index()\n",
    "        if normalize: df['Time'] = df['Time'].div(nclasses)\n",
    "        f, (ax1, ax2) = plt.subplots(2, 1, figsize=figsize, sharex=True)\n",
    "        for s in df[group].unique():\n",
    "            df.loc[(df[group]==s),'Time'] = df.loc[(df[group]==s),'Time'].div(df[(df[group]==s) & (df['Approach']=='scratch')]['Time'].values[0])\n",
    "        df = df[~(df['Approach']=='scratch')]\n",
    "        describe = df.groupby('Approach')['Time'].describe()[['mean','std']].reset_index()\n",
    "        describe[describe.Approach.isin(ord_list)].to_csv(type+'_'+str(nclasses)+'.csv')\n",
    "        if nclasses==20:\n",
    "            bars1 = sns.barplot(x='Approach', y='Time' , data=df, ax=ax1, order=ord_list, palette=palette)#, ci='sd')\n",
    "            bars2 = sns.barplot(x='Approach', y='Time' , data=df, ax=ax2, order=ord_list, palette=palette)#, ci='sd')            \n",
    "            ax1.set_ylim(9.5, 11.5)  \n",
    "            ax2.set_ylim(0, 1.1)  \n",
    "            ymax = 20\n",
    "            divided_barplot_settings(ax1,ax2,ymax)\n",
    "            add_vertical_lines_divided(ax1,ax2,ymax,ord_list)\n",
    "            ax1.yaxis.grid(True)\n",
    "            ax1.set_axisbelow(True)\n",
    "            ax2.yaxis.grid(True)\n",
    "            ax2.set_axisbelow(True)\n",
    "            ax2.set_ylabel('Ratio of Scratch Total Training Time')\n",
    "            ax2.yaxis.set_label_coords(-0.10,1.02)                                             \n",
    "            ax2.set_xticklabels(labels)\n",
    "        else:\n",
    "            plt.close()\n",
    "            bars1 = sns.barplot(x='Approach', y='Time' , data=df, ax=ax, order=ord_list, palette=palette)#, ci='sd')\n",
    "            ax.set_ylim(0, 0.06)\n",
    "            ax.set_ylabel('Ratio of Scratch Total Training Time')\n",
    "            ymax=20\n",
    "            add_vertical_lines(ax, ymax, ord_list)\n",
    "            ax.set_xticklabels(labels)\n",
    "            \n",
    "        plt.xticks(rotation=45)\n",
    "        ax.set_xlabel('')\n",
    "        ax.yaxis.grid(True)\n",
    "        ax.set_axisbelow(True)\n",
    "        sns.reset_defaults() \n",
    "\n",
    "    elif type=='stacked':\n",
    "        df = df_ti[(df_ti['Task']==appr_task) & (df_ti['Type']!='test') & (df_ti['Approach'].isin(ord_list))]\n",
    "        df_total = df_ti[(df_ti['Task']==appr_task) & (df_ti['Type']!='test')].groupby(['Approach',group])['Time'].sum().reset_index()\n",
    "        times={}\n",
    "        t = ['post_train', 'train', 'pre_train']\n",
    "        for time_type in t:\n",
    "            df_temp= df[df['Type']==time_type].reset_index()\n",
    "            for s in df_temp[group].unique():\n",
    "                df_temp.loc[(df_temp[group]==s),'Time'] = df_temp.loc[(df_temp[group]==s),'Time'].div(df_total[(df_total[group]==s) & (df_total['Approach']=='scratch')]['Time'].values[0])\n",
    "            df_temp = df_temp[~(df_temp['Approach']=='scratch')]\n",
    "            df_temp = df_temp.groupby('Approach')['Time'].mean().reset_index()\n",
    "            #new_row = {'Approach':'_', 'Time':0}\n",
    "            #df_temp = df_temp.append(new_row, ignore_index=True)\n",
    "            #df_temp = df_temp.append(new_row, ignore_index=True)\n",
    "            times[time_type] = df_temp['Time'].values  \n",
    "\n",
    "        approach = df_temp['Approach']\n",
    "        index = [appr_dict[i] for i in approach.values]\n",
    "        ord_list_new = [appr_dict[i] for i in ord_list]\n",
    "        df = pd.DataFrame({'Approach':index, 'Pre_Train':times['pre_train'], 'Train':times['train'], 'Post_Train':times['post_train'] })\n",
    "        df = df.set_index('Approach').loc[ord_list_new].reset_index()\n",
    "        #df = df.drop(labels=[2,9], axis=0).set_index('Approach')\n",
    "        print(df)\n",
    "        patch1 = mpatches.Patch(color='gainsboro', label='Post-SGD', ec='black',lw=1,hatch='////')\n",
    "        patch2 = mpatches.Patch(color='gainsboro', label='SGD', ec='black',lw=1)\n",
    "        handles = [patch1, patch2]\n",
    "        \n",
    "        if nclasses==1:\n",
    "            ax.set_ylabel('Ratio of Scratch Total Training Time', fontsize=13)\n",
    "            bars = df.plot.bar(stacked=True, ax=ax, alpha=.80, edgecolor='black',width=0.7, legend=True) \n",
    "            ymax=0.06\n",
    "            ax.set_xticklabels(labels, fontsize=11)\n",
    "            plt.xticks(rotation=45)   \n",
    "            plt.xlabel('')\n",
    "            plt.ylim([0,ymax])\n",
    "            ax.yaxis.grid(True, linestyle='--')\n",
    "            ax.set_axisbelow(True)\n",
    "            ax.legend(handles=handles, facecolor='white', ncol=1, frameon=True, handlelength=1.50, columnspacing=5, fontsize=12, handletextpad=0.5, numpoints=1)\n",
    "            add_vertical_lines_hatches_colors(ax,ymax,ord_list,palette)\n",
    "        else:\n",
    "            f, (ax1, ax2) = plt.subplots(2, 1, figsize=figsize, sharex=True)\n",
    "            ax2.set_ylim(0, 1.1)  #era 1.1\n",
    "            ax1.set_ylim(9.5, 10.6) #era 11\n",
    "            bars1 = df.plot.bar(stacked=True, ax=ax1, alpha=.80, edgecolor='black',width=0.7, legend=True) \n",
    "            bars2 = df.plot.bar(stacked=True, ax=ax2, alpha=.80, edgecolor='black',width=0.7, legend=False)                     \n",
    "            ax1.yaxis.grid(True, linestyle='--')\n",
    "            ax1.set_axisbelow(True)\n",
    "            ax2.yaxis.grid(True, linestyle='--')\n",
    "            ax2.set_axisbelow(True)\n",
    "            ymax = 20\n",
    "            divided_barplot_settings(ax1,ax2,ymax)\n",
    "            plt.xticks(rotation=45)\n",
    "            ax.set_xlabel('')\n",
    "            ax.yaxis.grid(True)\n",
    "            ax.set_axisbelow(True)\n",
    "            ax2.set_xticklabels(labels, fontsize=11)\n",
    "            ax2.set_ylabel('Ratio of Scratch Total Training Time', fontsize=13)\n",
    "            ax2.yaxis.set_label_coords(-0.12,1.02)\n",
    "            add_vertical_lines_hatches_colors_divided(ax1,ax2,ymax,ord_list,palette)\n",
    "            ax1.legend(handles=handles, facecolor='white', ncol=1, frameon=True, handlelength=1.50, columnspacing=5, fontsize=12, handletextpad=0.5, numpoints=1)\n",
    "\n",
    "        \n",
    "    elif type=='post_train_percentage':\n",
    "        dft1 = df_ti[(df_ti['Task']==appr_task) & (df_ti['Type']!='test')]\n",
    "        df1 = dft1.groupby(['Approach',group])['Time'].sum().reset_index()\n",
    "        df1['Time'] = df1['Time'].div(nclasses)\n",
    "        df_all = deepcopy(df1)\n",
    "        df_all = df_all.sort_values(['Approach', group]).reset_index()\n",
    "        sns.reset_defaults()\n",
    "        df1 = df_ti[(df_ti['Task']==appr_task) & (df_ti['Type']=='post_train')]\n",
    "        df1 = df1.sort_values(['Approach', group]).reset_index()\n",
    "        df=deepcopy(df1)\n",
    "        df['Time'] = df['Time'].div(nclasses).round(2)\n",
    "        df['Time'] = 100*df['Time'].div(df_all['Time'])\n",
    "        bars = sns.barplot(x='Approach', y='Time' , data=df, ax=ax, order=ord_list, palette=palette)#, ci='sd')\n",
    "        plt.xticks(rotation=45)\n",
    "        ax.set_xticklabels(labels)\n",
    "        plt.ylabel('Percentage of Total Training Time')\n",
    "        plt.xlabel('')\n",
    "        axx = plt.gca()\n",
    "        axx.yaxis.grid(True)\n",
    "        axx.set_axisbelow(True)\n",
    "        plt.ylim([0,40])\n",
    "        ymax=40\n",
    "        add_vertical_lines(axx,ymax,ord_list)\n",
    "        \n",
    "    else:\n",
    "        sns.reset_defaults()\n",
    "        df1 = df_ti[(df_ti['Task']==appr_task) & (df_ti['Type']==type)]\n",
    "        df1 = df1.sort_values(['Approach', group]).reset_index()\n",
    "        df=deepcopy(df1)\n",
    "        if normalize:\n",
    "            df['Time'] = df['Time'].div(nclasses).round(2)\n",
    "        else:\n",
    "            df['Time'] = df['Time'].round(2)\n",
    "        bars = sns.barplot(x='Approach', y='Time' , data=df, ax=ax, order=ord_list, palette=palette)#, ci='sd')\n",
    "        describe = df.groupby('Approach')['Time'].describe()[['mean','std']].reset_index()\n",
    "        describe[describe.Approach.isin(ord_list)].to_csv(type+'_'+str(nclasses)+'.csv')\n",
    "        plt.xticks(rotation=45)\n",
    "        ax.set_xticklabels(labels)\n",
    "        plt.ylabel('Time [s]')\n",
    "        plt.xlabel('')\n",
    "        axx = plt.gca()\n",
    "        bars.set_yscale(\"log\")\n",
    "        axx.yaxis.set_minor_locator(LogLocator(base=10, subs='auto'))\n",
    "        ymax= 110 if normalize else 110*nclasses\n",
    "        plt.ylim([0,ymax])\n",
    "        add_vertical_lines(axx,ymax,ord_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "fc1ed263",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 760/760 [00:11<00:00, 66.75it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 230/230 [00:01<00:00, 117.13it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1279/1279 [00:11<00:00, 113.41it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1410/1410 [00:01<00:00, 711.53it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 650/650 [00:01<00:00, 508.36it/s]\n"
     ]
    }
   ],
   "source": [
    "#20+20 times\n",
    "#load old experiments for info about epochs\n",
    "exp_name = 'base20_incr20_stop2'\n",
    "results_path = '/media/nas/datasets/MIRAGE_2020/FSCIL_approaches/hf-project/final_results/results/'\n",
    "df_stdout_filenames = glob('%s/*%s*/stdout*' % (results_path, exp_name), recursive=True)\n",
    "df_stdout_filenames = discard_duplicates(df_stdout_filenames)\n",
    "df_tr, df_ti = get_training_info(df_stdout_filenames)\n",
    "df_tr = df_tr[df_tr['Seed']!=0]\n",
    "df_ti = df_ti[df_ti['Seed']!=0]\n",
    "#load old scratch experiments for info about epochs\n",
    "exp_name = 'mirage_generic_scratch'\n",
    "results_path = '/media/nas/datasets/MIRAGE_2020/FSCIL_approaches/hf-project/final_results/results_UB/'\n",
    "df_stdout_filenames = glob('%s/*%s*/stdout*' % (results_path, exp_name), recursive=True)\n",
    "df_stdout_filenames = discard_duplicates(df_stdout_filenames)\n",
    "_df_tr_scratch, _df_ti_scratch = get_training_info(df_stdout_filenames)\n",
    "_df_tr_scratch = _df_tr_scratch[_df_tr_scratch['Base Apps'].isin([20, 40])]\n",
    "_df_ti_scratch = _df_ti_scratch[_df_ti_scratch['Base Apps'].isin([20, 40])]\n",
    "_df_tr_scratch.loc[_df_tr_scratch['Base Apps'] == 20, 'Task'] = 0\n",
    "_df_ti_scratch.loc[_df_ti_scratch['Base Apps'] == 20, 'Task'] = 0\n",
    "_df_tr_scratch.loc[_df_tr_scratch['Base Apps'] == 40, 'Task'] = 1\n",
    "_df_ti_scratch.loc[_df_ti_scratch['Base Apps'] == 40, 'Task'] = 1\n",
    "#load new experiments (controlled)\n",
    "exp_name = 'timing_base20'\n",
    "results_path = '/media/nas/datasets/MIRAGE_2020/FSCIL_approaches/hf-project/results/'\n",
    "df_stdout_filenames = glob('%s/*%s*/stdout*' % (results_path, exp_name), recursive=True)\n",
    "df_stdout_filenames = discard_duplicates(df_stdout_filenames)\n",
    "_df_tr, _df_ti = get_training_info(df_stdout_filenames)\n",
    "\n",
    "_df_tr = _df_tr[_df_tr['Last App']==-1]\n",
    "_df_tr = _df_tr[_df_tr['Seed']!=0]\n",
    "_df_ti = _df_ti[_df_ti['Seed']!=0]\n",
    "\n",
    "_df_tr, df_tr, _df_ti = clean_results(_df_tr, df_tr, _df_ti)\n",
    "\n",
    "final_df = pd.concat((df_tr[df_tr['Task'] == 1], _df_tr_scratch[_df_tr_scratch['Task'] == 1]), axis=0)\n",
    "final_df = final_df[final_df['Retrained App']==-1]\n",
    "\n",
    "mean_time_per_epoch = _df_tr[_df_tr['Task'] == 1].groupby(['Seed', 'Approach', 'Last App', 'First Momentum',\n",
    "                        'Base Momentum', 'Network', 'Out Features Size', 'Model App'])['Train Time'].mean().reset_index()\n",
    "\n",
    "max_epochs = final_df.groupby(['Seed', 'Approach', 'Last App', 'First Momentum', 'Base Momentum', \n",
    "                               'Network', 'Out Features Size', 'Model App'])['Epoch'].max().reset_index()\n",
    "\n",
    "\n",
    "columns = ['Approach', 'Seed', 'Model App']\n",
    "groups = list(mean_time_per_epoch.groupby(columns).groups)\n",
    "for group in tqdm(groups):\n",
    "\n",
    "    _filter = reduce(lambda x, y: x & y, [mean_time_per_epoch[col] == g for col, g in zip(columns, group)])\n",
    "    train_time = mean_time_per_epoch.loc[_filter]['Train Time'].values[0]\n",
    "    _filter = reduce(lambda x, y: x & y, [max_epochs[col] == g for col, g in zip(columns, group)])\n",
    "    if sum(_filter) == 0:\n",
    "        continue\n",
    "    \n",
    "    max_epochs.loc[_filter, 'Train Time'] = train_time\n",
    "max_epochs['Total Time'] = max_epochs[['Epoch', 'Train Time']].apply(lambda x: x[0] * x[1], axis=1)\n",
    "\n",
    "max_epochs = max_epochs.groupby(['Approach', 'Seed']).sum().reset_index()\n",
    "\n",
    "groups = list(_df_ti.groupby(['Approach', 'Seed', 'Last App']).groups)\n",
    "\n",
    "failed_apprs = []\n",
    "columns = ['Approach', 'Seed'] \n",
    "i = 0\n",
    "for group in tqdm(groups):\n",
    "    _group = group\n",
    "    _filter = reduce(lambda x, y: x & y, [max_epochs[col] == g for col, g in zip(columns, _group)])\n",
    "    if sum(_filter) == 0:\n",
    "        failed_apprs.append(group[0])\n",
    "        continue\n",
    "    t = max_epochs.loc[_filter, 'Total Time'].values[0]\n",
    "    \n",
    "    _filter = reduce(lambda x, y: x & y, [_df_ti[col] == g for col, g in zip(columns, group)])\n",
    "    _df_ti.loc[(_filter) & (_df_ti['Type'] == 'train') & (_df_ti['Task'] == 1), 'Time'] = t\n",
    "\n",
    "    \n",
    "#correzione chen2021 post_train\n",
    "df_tr_chen = df_tr[(df_tr['Approach']=='chen2021') & (df_tr['Retrained App']!=-1)]\n",
    "temp = df_tr_chen.groupby(['Seed', 'Retrained App'])['Epoch'].max().reset_index()\n",
    "temp_time = mean_time_per_epoch.groupby(['Seed', 'Approach'])['Train Time'].mean().reset_index()\n",
    "temp_time = temp_time[temp_time['Approach']=='chen2021']\n",
    "temp = pd.merge(temp, temp_time, on=[\"Seed\"])\n",
    "temp['Retraining Time'] = temp[['Epoch', 'Train Time']].apply(lambda x: x[0] * x[1], axis=1)\n",
    "temp = pd.merge(temp[['Retraining Time', 'Seed']], _df_ti[(_df_ti['Approach']=='chen2021') &\n",
    "    (_df_ti['Type']=='post_train') & (_df_ti['Task']==1)],on=\"Seed\")\n",
    "temp['Time'] = temp[['Retraining Time', 'Time']].apply(lambda x: x[0] + x[1], axis=1)\n",
    "\n",
    "for c in temp['Seed'].unique():\n",
    "    _df_ti.loc[_df_ti[(_df_ti['Approach']=='chen2021') & (_df_ti['Type']=='post_train') & (_df_ti['Task']==1) & (_df_ti['Seed']==c)].index.values[0], 'Time'] = temp[temp['Seed']==c]['Time'].values[0]\n",
    "    \n",
    "#correzione eeil train\n",
    "_df_ti = eeil_correction(_df_ti, group='Seed')\n",
    "\n",
    "_df_ti.loc[_df_ti['Approach']=='joint', 'Approach'] = 'jointmem'\n",
    "_df_ti.loc[_df_ti['Approach']=='backbonefreezing', 'Approach'] = 'backbonefreezingmem'\n",
    "_df_ti.loc[_df_ti['Approach']=='lwf','Approach'] = 'lwfgkd'\n",
    "\n",
    "_df_ti_20 = deepcopy(_df_ti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "5b8111a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 760/760 [00:12<00:00, 62.45it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 230/230 [00:02<00:00, 102.62it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1279/1279 [00:11<00:00, 115.06it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1410/1410 [00:01<00:00, 714.08it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 650/650 [00:01<00:00, 510.10it/s]\n"
     ]
    }
   ],
   "source": [
    "#20+20 times min cpu\n",
    "#load old experiments for info about epochs\n",
    "exp_name = 'base20_incr20_stop2'\n",
    "results_path = '/media/nas/datasets/MIRAGE_2020/FSCIL_approaches/hf-project/final_results/results/'\n",
    "df_stdout_filenames = glob('%s/*%s*/stdout*' % (results_path, exp_name), recursive=True)\n",
    "df_stdout_filenames = discard_duplicates(df_stdout_filenames)\n",
    "df_tr, df_ti = get_training_info(df_stdout_filenames)\n",
    "df_tr = df_tr[df_tr['Seed']!=0]\n",
    "df_ti = df_ti[df_ti['Seed']!=0]\n",
    "#load old scratch experiments for info about epochs\n",
    "exp_name = 'mirage_generic_scratch'\n",
    "results_path = '/media/nas/datasets/MIRAGE_2020/FSCIL_approaches/hf-project/final_results/results_UB/'\n",
    "df_stdout_filenames = glob('%s/*%s*/stdout*' % (results_path, exp_name), recursive=True)\n",
    "df_stdout_filenames = discard_duplicates(df_stdout_filenames)\n",
    "_df_tr_scratch, _df_ti_scratch = get_training_info(df_stdout_filenames)\n",
    "_df_tr_scratch = _df_tr_scratch[_df_tr_scratch['Base Apps'].isin([20, 40])]\n",
    "_df_ti_scratch = _df_ti_scratch[_df_ti_scratch['Base Apps'].isin([20, 40])]\n",
    "_df_tr_scratch.loc[_df_tr_scratch['Base Apps'] == 20, 'Task'] = 0\n",
    "_df_ti_scratch.loc[_df_ti_scratch['Base Apps'] == 20, 'Task'] = 0\n",
    "_df_tr_scratch.loc[_df_tr_scratch['Base Apps'] == 40, 'Task'] = 1\n",
    "_df_ti_scratch.loc[_df_ti_scratch['Base Apps'] == 40, 'Task'] = 1\n",
    "#load new experiments (controlled)\n",
    "exp_name = 'timing_base20'\n",
    "results_path = '/media/nas/datasets/MIRAGE_2020/FSCIL_approaches/hf-project/results/'\n",
    "df_stdout_filenames = glob('%s/*%s*/stdout*' % (results_path, exp_name), recursive=True)\n",
    "df_stdout_filenames = discard_duplicates(df_stdout_filenames)\n",
    "_df_tr, _df_ti = get_training_info(df_stdout_filenames)\n",
    "\n",
    "_df_tr = _df_tr[_df_tr['Last App']==-1]\n",
    "_df_tr = _df_tr[_df_tr['Seed']!=0]\n",
    "_df_ti = _df_ti[_df_ti['Seed']!=0]\n",
    "\n",
    "_df_tr, df_tr, _df_ti = clean_results(_df_tr, df_tr, _df_ti)\n",
    "\n",
    "final_df = pd.concat((df_tr[df_tr['Task'] == 1], _df_tr_scratch[_df_tr_scratch['Task'] == 1]), axis=0)\n",
    "final_df = final_df[final_df['Retrained App']==-1]\n",
    "\n",
    "mean_time_per_epoch = _df_tr[_df_tr['Task'] == 1].groupby(['Seed', 'Approach', 'Last App', 'First Momentum',\n",
    "                        'Base Momentum', 'Network', 'Out Features Size', 'Model App'])['Train Time'].min().reset_index()\n",
    "\n",
    "max_epochs = final_df.groupby(['Seed', 'Approach', 'Last App', 'First Momentum', 'Base Momentum', \n",
    "                               'Network', 'Out Features Size', 'Model App'])['Epoch'].max().reset_index()\n",
    "\n",
    "columns = ['Approach', 'Seed', 'Model App']\n",
    "groups = list(mean_time_per_epoch.groupby(columns).groups)\n",
    "for group in tqdm(groups):\n",
    "\n",
    "    _filter = reduce(lambda x, y: x & y, [mean_time_per_epoch[col] == g for col, g in zip(columns, group)])\n",
    "    train_time = mean_time_per_epoch.loc[_filter]['Train Time'].values[0]\n",
    "    _filter = reduce(lambda x, y: x & y, [max_epochs[col] == g for col, g in zip(columns, group)])\n",
    "    if sum(_filter) == 0:\n",
    "        continue\n",
    "    \n",
    "    max_epochs.loc[_filter, 'Train Time'] = train_time\n",
    "max_epochs['Total Time'] = max_epochs[['Epoch', 'Train Time']].apply(lambda x: x[0] * x[1], axis=1)\n",
    "\n",
    "max_epochs = max_epochs.groupby(['Approach', 'Seed']).sum().reset_index()\n",
    "\n",
    "groups = list(_df_ti.groupby(['Approach', 'Seed', 'Last App']).groups)\n",
    "\n",
    "failed_apprs = []\n",
    "columns = ['Approach', 'Seed'] \n",
    "i = 0\n",
    "for group in tqdm(groups):\n",
    "    _group = group\n",
    "    _filter = reduce(lambda x, y: x & y, [max_epochs[col] == g for col, g in zip(columns, _group)])\n",
    "    if sum(_filter) == 0:\n",
    "        failed_apprs.append(group[0])\n",
    "        continue\n",
    "    t = max_epochs.loc[_filter, 'Total Time'].values[0]\n",
    "    \n",
    "    _filter = reduce(lambda x, y: x & y, [_df_ti[col] == g for col, g in zip(columns, group)])\n",
    "    _df_ti.loc[(_filter) & (_df_ti['Type'] == 'train') & (_df_ti['Task'] == 1), 'Time'] = t\n",
    "\n",
    "    \n",
    "#correzione chen2021 post_train\n",
    "df_tr_chen = df_tr[(df_tr['Approach']=='chen2021') & (df_tr['Retrained App']!=-1)]\n",
    "temp = df_tr_chen.groupby(['Seed', 'Retrained App'])['Epoch'].max().reset_index()\n",
    "temp_time = mean_time_per_epoch.groupby(['Seed', 'Approach'])['Train Time'].min().reset_index()\n",
    "temp_time = temp_time[temp_time['Approach']=='chen2021']\n",
    "temp = pd.merge(temp, temp_time, on=[\"Seed\"])\n",
    "temp['Retraining Time'] = temp[['Epoch', 'Train Time']].apply(lambda x: x[0] * x[1], axis=1)\n",
    "temp = pd.merge(temp[['Retraining Time', 'Seed']], _df_ti[(_df_ti['Approach']=='chen2021') &\n",
    "    (_df_ti['Type']=='post_train') & (_df_ti['Task']==1)],on=\"Seed\")\n",
    "temp['Time'] = temp[['Retraining Time', 'Time']].apply(lambda x: x[0] + x[1], axis=1)\n",
    "\n",
    "for c in temp['Seed'].unique():\n",
    "    _df_ti.loc[_df_ti[(_df_ti['Approach']=='chen2021') & (_df_ti['Type']=='post_train') & (_df_ti['Task']==1) & (_df_ti['Seed']==c)].index.values[0], 'Time'] = temp[temp['Seed']==c]['Time'].values[0]\n",
    "    \n",
    "#correzione eeil train\n",
    "_df_ti = eeil_correction_mincpu(_df_ti, group='Seed')\n",
    "\n",
    "_df_ti.loc[_df_ti['Approach']=='joint', 'Approach'] = 'jointmem'\n",
    "_df_ti.loc[_df_ti['Approach']=='backbonefreezing', 'Approach'] = 'backbonefreezingmem'\n",
    "_df_ti.loc[_df_ti['Approach']=='lwf','Approach'] = 'lwfgkd'\n",
    "\n",
    "_df_ti_20_mincpu = deepcopy(_df_ti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "8ed55a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 760/760 [00:12<00:00, 61.67it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 230/230 [00:02<00:00, 93.33it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1279/1279 [00:11<00:00, 115.95it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1410/1410 [00:01<00:00, 720.43it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 650/650 [00:01<00:00, 634.35it/s]\n"
     ]
    }
   ],
   "source": [
    "#20+20 times min gpu\n",
    "#load old experiments for info about epochs\n",
    "exp_name = 'base20_incr20_stop2'\n",
    "results_path = '/media/nas/datasets/MIRAGE_2020/FSCIL_approaches/hf-project/final_results/results/'\n",
    "df_stdout_filenames = glob('%s/*%s*/stdout*' % (results_path, exp_name), recursive=True)\n",
    "df_stdout_filenames = discard_duplicates(df_stdout_filenames)\n",
    "df_tr, df_ti = get_training_info(df_stdout_filenames)\n",
    "df_tr = df_tr[df_tr['Seed']!=0]\n",
    "df_ti = df_ti[df_ti['Seed']!=0]\n",
    "#load old scratch experiments for info about epochs\n",
    "exp_name = 'mirage_generic_scratch'\n",
    "results_path = '/media/nas/datasets/MIRAGE_2020/FSCIL_approaches/hf-project/final_results/results_UB/'\n",
    "df_stdout_filenames = glob('%s/*%s*/stdout*' % (results_path, exp_name), recursive=True)\n",
    "df_stdout_filenames = discard_duplicates(df_stdout_filenames)\n",
    "_df_tr_scratch, _df_ti_scratch = get_training_info(df_stdout_filenames)\n",
    "_df_tr_scratch = _df_tr_scratch[_df_tr_scratch['Base Apps'].isin([20, 40])]\n",
    "_df_ti_scratch = _df_ti_scratch[_df_ti_scratch['Base Apps'].isin([20, 40])]\n",
    "_df_tr_scratch.loc[_df_tr_scratch['Base Apps'] == 20, 'Task'] = 0\n",
    "_df_ti_scratch.loc[_df_ti_scratch['Base Apps'] == 20, 'Task'] = 0\n",
    "_df_tr_scratch.loc[_df_tr_scratch['Base Apps'] == 40, 'Task'] = 1\n",
    "_df_ti_scratch.loc[_df_ti_scratch['Base Apps'] == 40, 'Task'] = 1\n",
    "\n",
    "\n",
    "exp_name = 'timing_base20'\n",
    "results_path = '/media/nas/datasets/MIRAGE_2020/FSCIL_approaches/hf-project/results/'\n",
    "df_stdout_filenames = glob('%s/*%s*/stdout*' % (results_path, exp_name), recursive=True)\n",
    "df_stdout_filenames = discard_duplicates(df_stdout_filenames)\n",
    "_df_tr, _df_ti = get_training_info(df_stdout_filenames)\n",
    "\n",
    "_df_tr = _df_tr[_df_tr['Last App']==-1]\n",
    "_df_tr = _df_tr[_df_tr['Seed']!=0]\n",
    "_df_ti = _df_ti[_df_ti['Seed']!=0]\n",
    "\n",
    "       \n",
    "_df_tr, df_tr, _df_ti = clean_results(_df_tr, df_tr, _df_ti)\n",
    "\n",
    "t = df_ti[~((df_ti['Approach']=='bic') & (df_ti['Memory Size'] != 1000))]\n",
    "df_ti = deepcopy(t)\n",
    "\n",
    "\n",
    "appr_list = ['joint', 'backbonefreezing' ]\n",
    "t = df_ti[~((df_ti['Approach'].isin(appr_list)) & (df_ti['Memory Size'] == 0))]\n",
    "df_ti = deepcopy(t)\n",
    "\n",
    "final_df = pd.concat((df_tr[df_tr['Task'] == 1], _df_tr_scratch[_df_tr_scratch['Task'] == 1]), axis=0)\n",
    "final_df = final_df[final_df['Retrained App']==-1]\n",
    "\n",
    "final_gpu_time = pd.concat((df_ti[df_ti['Task'] == 1], _df_ti_scratch[_df_ti_scratch['Task'] == 1]), axis=0)\n",
    "\n",
    "t = final_gpu_time[~((final_gpu_time['Approach']=='scratch') & (final_gpu_time['Memory Size'] !=0))]\n",
    "final_gpu_time = deepcopy(t)\n",
    "\n",
    "\n",
    "mean_time_per_epoch = df_tr[df_tr['Task'] == 1].groupby(['Seed', 'Approach', 'Last App', 'First Momentum',\n",
    "                        'Base Momentum', 'Network', 'Out Features Size', 'Model App'])['Train Time'].min().reset_index()\n",
    "mean_time_per_epoch_scratch = _df_tr_scratch[(_df_tr_scratch['Approach'] == 'scratch') & (_df_tr_scratch['Task'] == 1)].groupby(['Seed', 'Approach',                                                                                                                                 'Last App', 'First Momentum',\n",
    "                        'Base Momentum', 'Network', 'Out Features Size', 'Model App'])['Train Time'].min().reset_index()\n",
    "mean_time_per_epoch = pd.concat([mean_time_per_epoch_scratch, mean_time_per_epoch])\n",
    "\n",
    "\n",
    "max_epochs = final_df.groupby(['Seed', 'Approach', 'Last App', 'First Momentum', 'Base Momentum', \n",
    "                               'Network', 'Out Features Size', 'Model App'])['Epoch'].max().reset_index()\n",
    "\n",
    "columns = ['Approach', 'Seed', 'Model App']\n",
    "groups = list(mean_time_per_epoch.groupby(columns).groups)\n",
    "for group in tqdm(groups):\n",
    "\n",
    "    _filter = reduce(lambda x, y: x & y, [mean_time_per_epoch[col] == g for col, g in zip(columns, group)])\n",
    "    train_time = mean_time_per_epoch.loc[_filter]['Train Time'].values[0]\n",
    "    _filter = reduce(lambda x, y: x & y, [max_epochs[col] == g for col, g in zip(columns, group)])\n",
    "    if sum(_filter) == 0:\n",
    "        continue\n",
    "    \n",
    "    max_epochs.loc[_filter, 'Train Time'] = train_time\n",
    "max_epochs['Total Time'] = max_epochs[['Epoch', 'Train Time']].apply(lambda x: x[0] * x[1], axis=1)\n",
    "\n",
    "max_epochs = max_epochs.groupby(['Approach', 'Seed']).sum().reset_index()\n",
    "\n",
    "groups = list(final_gpu_time.groupby(['Approach', 'Seed', 'Last App']).groups)\n",
    "\n",
    "failed_apprs = []\n",
    "columns = ['Approach', 'Seed'] \n",
    "i = 0\n",
    "for group in tqdm(groups):\n",
    "    _group = group\n",
    "    _filter = reduce(lambda x, y: x & y, [max_epochs[col] == g for col, g in zip(columns, _group)])\n",
    "    if sum(_filter) == 0:\n",
    "        failed_apprs.append(group[0])\n",
    "        continue\n",
    "    t = max_epochs.loc[_filter, 'Total Time'].values[0]    \n",
    "    _filter = reduce(lambda x, y: x & y, [final_gpu_time[col] == g for col, g in zip(columns, group)])\n",
    "    final_gpu_time.loc[(_filter) & (final_gpu_time['Type'] == 'train') & (final_gpu_time['Task'] == 1), 'Time'] = t\n",
    "\n",
    "    \n",
    "#correzione chen2021 post_train\n",
    "df_tr_chen = df_tr[(df_tr['Approach']=='chen2021') & (df_tr['Retrained App']!=-1)]\n",
    "temp = df_tr_chen.groupby(['Seed', 'Retrained App'])['Epoch'].max().reset_index()\n",
    "temp_time = mean_time_per_epoch.groupby(['Seed', 'Approach'])['Train Time'].min().reset_index()\n",
    "temp_time = temp_time[temp_time['Approach']=='chen2021']\n",
    "temp = pd.merge(temp, temp_time, on=[\"Seed\"])\n",
    "temp['Retraining Time'] = temp[['Epoch', 'Train Time']].apply(lambda x: x[0] * x[1], axis=1)\n",
    "temp = pd.merge(temp[['Retraining Time', 'Seed']], _df_ti[(_df_ti['Approach']=='chen2021') &\n",
    "    (_df_ti['Type']=='post_train') & (_df_ti['Task']==1)],on=\"Seed\")\n",
    "temp['Time'] = temp[['Retraining Time', 'Time']].apply(lambda x: x[0] + x[1], axis=1)\n",
    "\n",
    "for c in temp['Seed'].unique():\n",
    "    final_gpu_time.loc[_df_ti[(_df_ti['Approach']=='chen2021') & (_df_ti['Type']=='post_train') & (_df_ti['Task']==1) & (_df_ti['Seed']==c)].index.values[0], 'Time'] = temp[temp['Seed']==c]['Time'].values[0]\n",
    "    \n",
    "#correzione eeil train\n",
    "final_gpu_time = eeil_correction_mingpu(final_gpu_time, group='Seed')\n",
    "\n",
    "final_gpu_time.loc[final_gpu_time['Approach']=='joint', 'Approach'] = 'jointmem'\n",
    "final_gpu_time.loc[final_gpu_time['Approach']=='backbonefreezing', 'Approach'] = 'backbonefreezingmem'\n",
    "final_gpu_time.loc[final_gpu_time['Approach']=='lwf','Approach'] = 'lwfgkd'\n",
    "\n",
    "_df_ti_20_mingpu = deepcopy(final_gpu_time)\n",
    "#_df_ti_20_mingpu.loc[_df_ti_20_mingpu.Approach=='scratch','Task'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "90edd657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Approach     Pre_Train     Train    Post_Train\n",
      "0       BiC  4.266354e-05  0.728858  1.018616e-02\n",
      "1      EEIL  0.000000e+00  0.680181  2.708990e-02\n",
      "2       EWC  0.000000e+00  0.691616  1.151539e-02\n",
      "3    FT-Mem  0.000000e+00  0.432946  8.598948e-03\n",
      "4    FZ-Mem  0.000000e+00  0.266974  8.720079e-03\n",
      "5     iCaRL  0.000000e+00  0.797528  8.786125e-03\n",
      "6    iCaRL+  0.000000e+00  0.717483  8.673433e-03\n",
      "7      IL2M  0.000000e+00  0.480197  1.010831e-02\n",
      "8     LUCIR  3.264089e-07  0.797476  8.711970e-03\n",
      "9   LwF-GKD  0.000000e+00  0.700408  8.131872e-07\n",
      "10  OvA-Ens  0.000000e+00  9.773782  3.545583e-01\n",
      "11     SSIL  0.000000e+00  0.277570  8.678744e-03\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n#20+20 plot times min gpu\\ntypes = ['all', 'post_train','post_train_percentage']\\nfor t in types:\\n    figsize = (7, 4)\\n    fig, ax = plt.subplots(figsize=figsize)\\n    plot_train_time(_df_ti_20_mingpu, t, appr_task = 1, group='Seed', nclasses = 20, normalize=True, figsize=figsize)\\n    t=t+'_normalized'\\n    plt.savefig('ExecTime_Scenario_20+20_%s_mingpu.pdf' % t, bbox_inches='tight')\\n    \\n    \\ntypes = ['stacked','all', 'post_train','post_train_percentage','all_normalized_scratch']\\nfor t in types:\\n    figsize = (7, 4)\\n    fig, ax = plt.subplots(figsize=figsize)\\n    plot_train_time(_df_ti_20_mingpu, t, appr_task=1, group='Seed', nclasses=20, normalize=False, figsize=figsize)\\n    plt.savefig('ExecTime_Scenario_20+20_%s_mingpu.pdf' % t, bbox_inches='tight')\\n\""
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#20+20 plot times\n",
    "'''\n",
    "types = ['all', 'post_train','post_train_percentage']\n",
    "for t in types:\n",
    "    figsize = (7, 4)\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    plot_train_time(_df_ti_20, t, appr_task = 1, group='Seed', nclasses = 20, normalize=True, figsize=figsize)\n",
    "    t=t+'_normalized'\n",
    "    plt.savefig('ExecTime_Scenario_20+20_%s.pdf' % t, bbox_inches='tight')\n",
    "    \n",
    "    \n",
    "types = ['stacked','all', 'post_train','post_train_percentage','all_normalized_scratch']\n",
    "for t in types:\n",
    "    figsize = (7, 4)\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    plot_train_time(_df_ti_20, t, appr_task=1, group='Seed', nclasses=20, normalize=False, figsize=figsize)\n",
    "    plt.savefig('ExecTime_Scenario_20+20_%s.pdf' % t, bbox_inches='tight')\n",
    "'''\n",
    "\n",
    "#20+20 plot times min cpu\n",
    "types = ['all', 'post_train','post_train_percentage']\n",
    "for t in types:\n",
    "    figsize = (7, 4)\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    plot_train_time(_df_ti_20_mincpu, t, appr_task = 1, group='Seed', nclasses = 20, normalize=True, figsize=figsize)\n",
    "    t=t+'_normalized'\n",
    "    plt.savefig('ExecTime_Scenario_20+20_%s_mincpu.pdf' % t, bbox_inches='tight')\n",
    "    \n",
    "    \n",
    "types = ['stacked','all', 'post_train','post_train_percentage','all_normalized_scratch']\n",
    "for t in types:\n",
    "    figsize = (7, 4)\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    plot_train_time(_df_ti_20_mincpu, t, appr_task=1, group='Seed', nclasses=20, normalize=False, figsize=figsize)\n",
    "    plt.savefig('ExecTime_Scenario_20+20_%s_mincpu.pdf' % t, bbox_inches='tight')\n",
    "    \n",
    "'''\n",
    "#20+20 plot times min gpu\n",
    "types = ['all', 'post_train','post_train_percentage']\n",
    "for t in types:\n",
    "    figsize = (7, 4)\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    plot_train_time(_df_ti_20_mingpu, t, appr_task = 1, group='Seed', nclasses = 20, normalize=True, figsize=figsize)\n",
    "    t=t+'_normalized'\n",
    "    plt.savefig('ExecTime_Scenario_20+20_%s_mingpu.pdf' % t, bbox_inches='tight')\n",
    "    \n",
    "    \n",
    "types = ['stacked','all', 'post_train','post_train_percentage','all_normalized_scratch']\n",
    "for t in types:\n",
    "    figsize = (7, 4)\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    plot_train_time(_df_ti_20_mingpu, t, appr_task=1, group='Seed', nclasses=20, normalize=False, figsize=figsize)\n",
    "    plt.savefig('ExecTime_Scenario_20+20_%s_mingpu.pdf' % t, bbox_inches='tight')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "4517da20",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 760/760 [00:12<00:00, 62.52it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 887/887 [00:07<00:00, 113.05it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [00:00<00:00, 87.65it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 670/670 [00:00<00:00, 826.21it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 670/670 [00:01<00:00, 445.86it/s]\n"
     ]
    }
   ],
   "source": [
    "#39+1 times\n",
    "#load old experiments for info about epochs\n",
    "exp_name = '39_1_base39_incr1_stop2'\n",
    "results_path = '/media/nas/datasets/MIRAGE_2020/FSCIL_approaches/hf-project/final_results/results/'\n",
    "df_stdout_filenames = glob('%s/*%s*/stdout*' % (results_path, exp_name), recursive=True)\n",
    "df_stdout_filenames = discard_duplicates(df_stdout_filenames)\n",
    "df_tr, df_ti = get_training_info(df_stdout_filenames)\n",
    "#load new experiments (isolated)\n",
    "exp_name= 'timing_base39'\n",
    "results_path = '/media/nas/datasets/MIRAGE_2020/FSCIL_approaches/hf-project/results/'\n",
    "df_stdout_filenames = glob('%s/*%s*/stdout*' % (results_path, exp_name), recursive=True)\n",
    "df_stdout_filenames = discard_duplicates(df_stdout_filenames)\n",
    "_df_tr, _df_ti = get_training_info(df_stdout_filenames)\n",
    "_df_tr = _df_tr[_df_tr['Seed']==0]\n",
    "_df_tr = _df_tr[_df_tr['Last App']!=-1]\n",
    "_df_ti = _df_ti[_df_ti['Last App']!=-1]\n",
    "#load old scratch experiments for info about epochs\n",
    "exp_name = 'mirage_generic_scratch_39_ub_base39_incr1_stop1'\n",
    "results_path = '/media/nas/datasets/MIRAGE_2020/FSCIL_approaches/hf-project/final_results/results_UB/'\n",
    "df_stdout_filenames = glob('%s/*%s*/stdout*' % (results_path, exp_name), recursive=True)\n",
    "df_stdout_filenames = discard_duplicates(df_stdout_filenames)\n",
    "_df_tr_scratch, _df_ti_scratch = get_training_info(df_stdout_filenames)\n",
    "\n",
    "_df_tr, df_tr, _df_ti = clean_results(_df_tr, df_tr, _df_ti)\n",
    "\n",
    "final_df = pd.concat((df_tr[df_tr['Task'] == 1], _df_tr_scratch[_df_tr_scratch['Task'] == 0]), axis=0)\n",
    "final_df = final_df[final_df['Retrained App']==-1]\n",
    "\n",
    "cols = ['Seed', 'Approach', 'Last App', 'First Momentum', 'Base Momentum', 'Network', 'Out Features Size']\n",
    "mean_time_per_epoch = _df_tr[_df_tr['Task'] == 1].groupby(cols)['Train Time'].mean().reset_index()\n",
    "max_epochs = final_df.groupby(cols)['Epoch'].max().reset_index()\n",
    "\n",
    "columns = ['Approach', 'Seed', 'Last App']\n",
    "groups = list(mean_time_per_epoch.groupby(columns).groups)\n",
    "for group in tqdm(groups):\n",
    "    _filter = reduce(lambda x, y: x & y, [mean_time_per_epoch[col] == g for col, g in zip(columns, group)])\n",
    "    train_time = mean_time_per_epoch.loc[_filter]['Train Time'].values[0]\n",
    "    _filter = reduce(lambda x, y: x & y, [max_epochs[col] == g for col, g in zip(columns, group)])\n",
    "    if sum(_filter) == 0:\n",
    "        continue\n",
    "    max_epochs.loc[_filter, 'Train Time'] = train_time\n",
    "    \n",
    "max_epochs['Total Time'] = max_epochs[['Epoch', 'Train Time']].apply(lambda x: x[0] * x[1], axis=1)\n",
    "\n",
    "groups = list(_df_ti.groupby(columns).groups)\n",
    "\n",
    "failed_apprs = []\n",
    "i = 0\n",
    "for group in tqdm(groups):\n",
    "    _group = group\n",
    "    _filter = reduce(lambda x, y: x & y, [max_epochs[col] == g for col, g in zip(columns, _group)])\n",
    "    if sum(_filter) == 0:\n",
    "        failed_apprs.append(group[0])\n",
    "        continue\n",
    "    t = max_epochs.loc[_filter, 'Total Time'].values[0]\n",
    "    _filter = reduce(lambda x, y: x & y, [_df_ti[col] == g for col, g in zip(columns, group)])\n",
    "    _df_ti.loc[(_filter) & (_df_ti['Type'] == 'train') & (_df_ti['Task'] == 1), 'Time'] = t\n",
    "\n",
    "#correzione chen2021 retraining time\n",
    "max_epochs_chen = max_epochs[max_epochs['Approach'] == 'chen2021'][['Epoch', 'Train Time', 'Last App']]\n",
    "df_tr_chen = df_tr[(df_tr['Approach'] == 'chen2021') & (df_tr['Task'] == 1) & (df_tr['Retrained App'] != -1)]\n",
    "max_epochs_chen = max_epochs_chen[['Train Time', 'Last App']]\n",
    "temp = df_tr_chen.groupby(['Last App', 'Retrained App'])['Epoch'].max().reset_index().groupby(['Last App']).sum().reset_index()\n",
    "temp = pd.merge(temp, max_epochs_chen, on=\"Last App\")\n",
    "temp['Retraining Time'] = temp[['Epoch', 'Train Time']].apply(lambda x: x[0] * x[1], axis=1)\n",
    "temp = pd.merge(temp[['Retraining Time', 'Last App']], _df_ti[(_df_ti['Approach']=='chen2021') & (_df_ti['Type']=='post_train') & (_df_ti['Task']==1)],on=\"Last App\")\n",
    "temp['Time'] = temp[['Retraining Time', 'Time']].apply(lambda x: x[0] + x[1], axis=1)\n",
    "\n",
    "for c in temp['Last App'].unique():\n",
    "    _df_ti.loc[_df_ti[(_df_ti['Approach']=='chen2021') & (_df_ti['Type']=='post_train') &\n",
    "    (_df_ti['Task']==1) & (_df_ti['Last App']==c)].index.values[0],'Time'] = temp[temp['Last App']==c]['Time'].values[0]\n",
    "    \n",
    "#correzione eeil train\n",
    "_df_ti = eeil_correction(_df_ti, group='Last App')\n",
    "\n",
    "for c in _df_ti[_df_ti['Approach']=='ssil']['Last App'].unique():\n",
    "    _df_ti.loc[_df_ti[(_df_ti['Approach']=='ssil') & (_df_ti['Type']=='post_train')  &\n",
    "    (_df_ti['Task']==1) & (_df_ti['Last App']==c)].index.values[0],'Time'] = _df_ti[(_df_ti['Approach']=='ssil') \n",
    "    & (_df_ti['Type']=='post_train')  & (_df_ti['Task']==1) & (_df_ti['Last App']==c)]['Time'].values[0]-0.2\n",
    "    \n",
    "    _df_ti.loc[_df_ti[(_df_ti['Approach']=='icarlp') & (_df_ti['Type']=='post_train')  &\n",
    "    (_df_ti['Task']==1) & (_df_ti['Last App']==c)].index.values[0],'Time'] = _df_ti[(_df_ti['Approach']=='icarlp') \n",
    "    & (_df_ti['Type']=='post_train')  & (_df_ti['Task']==1) & (_df_ti['Last App']==c)]['Time'].values[0]-0.2\n",
    "\n",
    "    \n",
    "_df_ti.loc[_df_ti['Approach']=='joint', 'Approach'] = 'jointmem'\n",
    "_df_ti.loc[_df_ti['Approach']=='backbonefreezing', 'Approach'] = 'backbonefreezingmem'\n",
    "_df_ti.loc[_df_ti['Approach']=='lwf','Approach'] = 'lwfgkd'\n",
    "\n",
    "_df_ti_39 = deepcopy(_df_ti)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "75f23828",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 760/760 [00:11<00:00, 63.69it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 887/887 [00:07<00:00, 122.42it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [00:00<00:00, 77.84it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 670/670 [00:00<00:00, 809.93it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 670/670 [00:01<00:00, 438.31it/s]\n"
     ]
    }
   ],
   "source": [
    "#min cpu * epochs on gpu\n",
    "#39+1 times\n",
    "#load old experiments for info about epochs\n",
    "exp_name = '39_1_base39_incr1_stop2'\n",
    "results_path = '/media/nas/datasets/MIRAGE_2020/FSCIL_approaches/hf-project/final_results/results/'\n",
    "df_stdout_filenames = glob('%s/*%s*/stdout*' % (results_path, exp_name), recursive=True)\n",
    "df_stdout_filenames = discard_duplicates(df_stdout_filenames)\n",
    "df_tr, df_ti = get_training_info(df_stdout_filenames)\n",
    "#load new experiments (isolated)\n",
    "exp_name= 'timing_base39'\n",
    "results_path = '/media/nas/datasets/MIRAGE_2020/FSCIL_approaches/hf-project/results/'\n",
    "df_stdout_filenames = glob('%s/*%s*/stdout*' % (results_path, exp_name), recursive=True)\n",
    "df_stdout_filenames = discard_duplicates(df_stdout_filenames)\n",
    "_df_tr, _df_ti = get_training_info(df_stdout_filenames)\n",
    "_df_tr = _df_tr[_df_tr['Seed']==0]\n",
    "_df_tr = _df_tr[_df_tr['Last App']!=-1]\n",
    "_df_ti = _df_ti[_df_ti['Last App']!=-1]\n",
    "#load old scratch experiments for info about epochs\n",
    "exp_name = 'mirage_generic_scratch_39_ub_base39_incr1_stop1'\n",
    "results_path = '/media/nas/datasets/MIRAGE_2020/FSCIL_approaches/hf-project/final_results/results_UB/'\n",
    "df_stdout_filenames = glob('%s/*%s*/stdout*' % (results_path, exp_name), recursive=True)\n",
    "df_stdout_filenames = discard_duplicates(df_stdout_filenames)\n",
    "_df_tr_scratch, _df_ti_scratch = get_training_info(df_stdout_filenames)\n",
    "\n",
    "_df_tr, df_tr, _df_ti = clean_results(_df_tr, df_tr, _df_ti)\n",
    "\n",
    "final_df = pd.concat((df_tr[df_tr['Task'] == 1], _df_tr_scratch[_df_tr_scratch['Task'] == 0]), axis=0)\n",
    "final_df = final_df[final_df['Retrained App']==-1]\n",
    "\n",
    "cols = ['Seed', 'Approach', 'Last App', 'First Momentum', 'Base Momentum', 'Network', 'Out Features Size']\n",
    "mean_time_per_epoch = _df_tr[_df_tr['Task'] == 1].groupby(cols)['Train Time'].min().reset_index()\n",
    "max_epochs = final_df.groupby(cols)['Epoch'].max().reset_index()\n",
    "\n",
    "#print(max_epochs[max_epochs['Approach']=='bic'][['Approach','Epoch','Last App']])\n",
    "\n",
    "columns = ['Approach', 'Seed', 'Last App']\n",
    "groups = list(mean_time_per_epoch.groupby(columns).groups)\n",
    "for group in tqdm(groups):\n",
    "    _filter = reduce(lambda x, y: x & y, [mean_time_per_epoch[col] == g for col, g in zip(columns, group)])\n",
    "    train_time = mean_time_per_epoch.loc[_filter]['Train Time'].values[0]\n",
    "    _filter = reduce(lambda x, y: x & y, [max_epochs[col] == g for col, g in zip(columns, group)])\n",
    "    if sum(_filter) == 0:\n",
    "        continue\n",
    "    max_epochs.loc[_filter, 'Train Time'] = train_time\n",
    "    \n",
    "max_epochs['Total Time'] = max_epochs[['Epoch', 'Train Time']].apply(lambda x: x[0] * x[1], axis=1)\n",
    "\n",
    "groups = list(_df_ti.groupby(columns).groups)\n",
    "\n",
    "failed_apprs = []\n",
    "i = 0\n",
    "for group in tqdm(groups):\n",
    "    _group = group\n",
    "    _filter = reduce(lambda x, y: x & y, [max_epochs[col] == g for col, g in zip(columns, _group)])\n",
    "    if sum(_filter) == 0:\n",
    "        failed_apprs.append(group[0])\n",
    "        continue\n",
    "    t = max_epochs.loc[_filter, 'Total Time'].values[0]\n",
    "    _filter = reduce(lambda x, y: x & y, [_df_ti[col] == g for col, g in zip(columns, group)])\n",
    "    _df_ti.loc[(_filter) & (_df_ti['Type'] == 'train') & (_df_ti['Task'] == 1), 'Time'] = t\n",
    "\n",
    "#correzione chen2021 retraining time\n",
    "max_epochs_chen = max_epochs[max_epochs['Approach'] == 'chen2021'][['Epoch', 'Train Time', 'Last App']]\n",
    "df_tr_chen = df_tr[(df_tr['Approach'] == 'chen2021') & (df_tr['Task'] == 1) & (df_tr['Retrained App'] != -1)]\n",
    "max_epochs_chen = max_epochs_chen[['Train Time', 'Last App']]\n",
    "temp = df_tr_chen.groupby(['Last App', 'Retrained App'])['Epoch'].max().reset_index().groupby(['Last App']).sum().reset_index()\n",
    "temp = pd.merge(temp, max_epochs_chen, on=\"Last App\")\n",
    "temp['Retraining Time'] = temp[['Epoch', 'Train Time']].apply(lambda x: x[0] * x[1], axis=1)\n",
    "temp = pd.merge(temp[['Retraining Time', 'Last App']], _df_ti[(_df_ti['Approach']=='chen2021') & (_df_ti['Type']=='post_train') & (_df_ti['Task']==1)],on=\"Last App\")\n",
    "temp['Time'] = temp[['Retraining Time', 'Time']].apply(lambda x: x[0] + x[1], axis=1)\n",
    "\n",
    "for c in temp['Last App'].unique():\n",
    "    _df_ti.loc[_df_ti[(_df_ti['Approach']=='chen2021') & (_df_ti['Type']=='post_train') &\n",
    "    (_df_ti['Task']==1) & (_df_ti['Last App']==c)].index.values[0],'Time'] = temp[temp['Last App']==c]['Time'].values[0]\n",
    "    \n",
    "#correzione eeil train\n",
    "_df_ti = eeil_correction_mincpu(_df_ti, group='Last App')\n",
    "\n",
    "for c in _df_ti[_df_ti['Approach']=='ssil']['Last App'].unique():\n",
    "    _df_ti.loc[_df_ti[(_df_ti['Approach']=='ssil') & (_df_ti['Type']=='post_train')  &\n",
    "    (_df_ti['Task']==1) & (_df_ti['Last App']==c)].index.values[0],'Time'] = _df_ti[(_df_ti['Approach']=='ssil') \n",
    "    & (_df_ti['Type']=='post_train')  & (_df_ti['Task']==1) & (_df_ti['Last App']==c)]['Time'].values[0]-0.2\n",
    "    \n",
    "    _df_ti.loc[_df_ti[(_df_ti['Approach']=='icarlp') & (_df_ti['Type']=='post_train')  &\n",
    "    (_df_ti['Task']==1) & (_df_ti['Last App']==c)].index.values[0],'Time'] = _df_ti[(_df_ti['Approach']=='icarlp') \n",
    "    & (_df_ti['Type']=='post_train')  & (_df_ti['Task']==1) & (_df_ti['Last App']==c)]['Time'].values[0]-0.2\n",
    "\n",
    "_df_ti.loc[_df_ti['Approach']=='joint', 'Approach'] = 'jointmem'\n",
    "_df_ti.loc[_df_ti['Approach']=='backbonefreezing', 'Approach'] = 'backbonefreezingmem'\n",
    "_df_ti.loc[_df_ti['Approach']=='lwf','Approach'] = 'lwfgkd'\n",
    "\n",
    "_df_ti_39_mincpu = deepcopy(_df_ti)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "7abb483e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 760/760 [00:11<00:00, 63.67it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 887/887 [00:07<00:00, 119.67it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [00:00<00:00, 79.96it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 640/640 [00:00<00:00, 789.12it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 670/670 [00:01<00:00, 535.66it/s]\n"
     ]
    }
   ],
   "source": [
    "#min gpu * epochs on gpu\n",
    "#39+1 times\n",
    "#load old experiments for info about epochs\n",
    "exp_name = '39_1_base39_incr1_stop2'\n",
    "results_path = '/media/nas/datasets/MIRAGE_2020/FSCIL_approaches/hf-project/final_results/results/'\n",
    "df_stdout_filenames = glob('%s/*%s*/stdout*' % (results_path, exp_name), recursive=True)\n",
    "df_stdout_filenames = discard_duplicates(df_stdout_filenames)\n",
    "df_tr, df_ti = get_training_info(df_stdout_filenames)\n",
    "\n",
    "#load new experiments (isolated)\n",
    "exp_name= 'timing_base39'\n",
    "results_path = '/media/nas/datasets/MIRAGE_2020/FSCIL_approaches/hf-project/results/'\n",
    "df_stdout_filenames = glob('%s/*%s*/stdout*' % (results_path, exp_name), recursive=True)\n",
    "df_stdout_filenames = discard_duplicates(df_stdout_filenames)\n",
    "_df_tr, _df_ti = get_training_info(df_stdout_filenames)\n",
    "_df_tr = _df_tr[_df_tr['Seed']==0]\n",
    "_df_tr = _df_tr[_df_tr['Last App']!=-1]\n",
    "_df_ti = _df_ti[_df_ti['Last App']!=-1]\n",
    "\n",
    "#load old scratch experiments for info about epochs\n",
    "exp_name = 'mirage_generic_scratch_39_ub_base39_incr1_stop1'\n",
    "results_path = '/media/nas/datasets/MIRAGE_2020/FSCIL_approaches/hf-project/final_results/results_UB/'\n",
    "df_stdout_filenames = glob('%s/*%s*/stdout*' % (results_path, exp_name), recursive=True)\n",
    "df_stdout_filenames = discard_duplicates(df_stdout_filenames)\n",
    "_df_tr_scratch, _df_ti_scratch = get_training_info(df_stdout_filenames)\n",
    "\n",
    "_df_tr, df_tr, _df_ti = clean_results(_df_tr, df_tr, _df_ti)\n",
    "\n",
    "final_df = pd.concat((df_tr[df_tr['Task'] == 1], _df_tr_scratch[_df_tr_scratch['Task'] == 0]), axis=0)\n",
    "final_df = final_df[final_df['Retrained App']==-1]\n",
    "\n",
    "appr_list = ['joint', 'backbonefreezing' ]\n",
    "t = df_ti[~((df_ti['Approach'].isin(appr_list)) & (df_ti['Memory Size'] == 0))]\n",
    "df_ti = deepcopy(t)\n",
    "\n",
    "final_gpu_time = pd.concat((df_ti[df_ti['Task'] == 1], _df_ti_scratch[_df_ti_scratch['Task'] == 0]), axis=0)\n",
    "max_epochs = final_df.groupby(cols)['Epoch'].max().reset_index()\n",
    "\n",
    "mean_time_per_epoch = df_tr[df_tr['Task'] == 1].groupby(cols)['Train Time'].min().reset_index()\n",
    "mean_time_per_epoch_scratch = _df_tr_scratch[(_df_tr_scratch['Approach'] == 'scratch') & (_df_tr_scratch['Task'] == 1)].groupby(cols)['Train Time'].min().reset_index()\n",
    "\n",
    "mean_time_per_epoch = pd.concat([mean_time_per_epoch_scratch, mean_time_per_epoch])\n",
    "\n",
    "cols = ['Seed', 'Approach', 'Last App', 'First Momentum', 'Base Momentum', 'Network', 'Out Features Size']\n",
    "\n",
    "columns = ['Approach', 'Seed', 'Last App']\n",
    "groups = list(mean_time_per_epoch.groupby(columns).groups)\n",
    "for group in tqdm(groups):\n",
    "    _filter = reduce(lambda x, y: x & y, [mean_time_per_epoch[col] == g for col, g in zip(columns, group)])\n",
    "    train_time = mean_time_per_epoch.loc[_filter]['Train Time'].values[0]\n",
    "    _filter = reduce(lambda x, y: x & y, [max_epochs[col] == g for col, g in zip(columns, group)])\n",
    "    if sum(_filter) == 0:\n",
    "        continue\n",
    "    max_epochs.loc[_filter, 'Train Time'] = train_time\n",
    "    \n",
    "max_epochs['Total Time'] = max_epochs[['Epoch', 'Train Time']].apply(lambda x: x[0] * x[1], axis=1)\n",
    "\n",
    "groups = list(_df_ti.groupby(columns).groups)\n",
    "\n",
    "failed_apprs = []\n",
    "i = 0\n",
    "for group in tqdm(groups):\n",
    "    _group = group\n",
    "    _filter = reduce(lambda x, y: x & y, [max_epochs[col] == g for col, g in zip(columns, _group)])\n",
    "    if sum(_filter) == 0:\n",
    "        failed_apprs.append(group[0])\n",
    "        continue\n",
    "    t = max_epochs.loc[_filter, 'Total Time'].values[0]\n",
    "    _filter = reduce(lambda x, y: x & y, [final_gpu_time[col] == g for col, g in zip(columns, group)])\n",
    "    if final_gpu_time.loc[(_filter)].Approach.unique()[0]!='scratch':\n",
    "        final_gpu_time.loc[(_filter) & (final_gpu_time['Type'] == 'train') & (final_gpu_time['Task'] == 1), 'Time'] = t\n",
    "    else:\n",
    "        final_gpu_time.loc[(_filter) & (final_gpu_time['Type'] == 'train') & (final_gpu_time['Task'] == 0), 'Time'] = t \n",
    "    \n",
    "#correzione chen2021 retraining time\n",
    "max_epochs_chen = max_epochs[max_epochs['Approach'] == 'chen2021'][['Epoch', 'Train Time', 'Last App']]\n",
    "df_tr_chen = df_tr[(df_tr['Approach'] == 'chen2021') & (df_tr['Task'] == 1) & (df_tr['Retrained App'] != -1)]\n",
    "max_epochs_chen = max_epochs_chen[['Train Time', 'Last App']]\n",
    "temp = df_tr_chen.groupby(['Last App', 'Retrained App'])['Epoch'].max().reset_index().groupby(['Last App']).sum().reset_index()\n",
    "temp = pd.merge(temp, max_epochs_chen, on=\"Last App\")\n",
    "temp['Retraining Time'] = temp[['Epoch', 'Train Time']].apply(lambda x: x[0] * x[1], axis=1)\n",
    "temp = pd.merge(temp[['Retraining Time', 'Last App']], _df_ti[(_df_ti['Approach']=='chen2021') & (_df_ti['Type']=='post_train') & (_df_ti['Task']==1)],on=\"Last App\")\n",
    "temp['Time'] = temp[['Retraining Time', 'Time']].apply(lambda x: x[0] + x[1], axis=1)\n",
    "\n",
    "for c in temp['Last App'].unique():\n",
    "    final_gpu_time.loc[final_gpu_time[(final_gpu_time['Approach']=='chen2021') & (final_gpu_time['Type']=='post_train') &\n",
    "    (final_gpu_time['Task']==1) & (final_gpu_time['Last App']==c)].index.values[0],'Time'] = temp[temp['Last App']==c]['Time'].values[0]\n",
    "    \n",
    "\n",
    "final_gpu_time.loc[final_gpu_time['Approach']=='joint', 'Approach'] = 'jointmem'\n",
    "final_gpu_time.loc[final_gpu_time['Approach']=='backbonefreezing', 'Approach'] = 'backbonefreezingmem'\n",
    "final_gpu_time.loc[final_gpu_time['Approach']=='lwf','Approach'] = 'lwfgkd'\n",
    "\n",
    "_df_ti_39_mingpu = deepcopy(final_gpu_time)    \n",
    "_df_ti_39_mingpu.loc[_df_ti_39_mingpu.Approach=='scratch','Task'] = 1\n",
    "\n",
    "#print(_df_ti_39_mingpu.groupby('Approach').describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "72c73909",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Approach     Pre_Train     Train    Post_Train\n",
      "0       BiC  5.544439e-06  0.046926  3.012566e-03\n",
      "1      EEIL  0.000000e+00  0.039611  1.192076e-02\n",
      "2       EWC  0.000000e+00  0.042776  5.389897e-04\n",
      "3    FT-Mem  0.000000e+00  0.027080  4.590553e-04\n",
      "4    FZ-Mem  0.000000e+00  0.014615  4.615652e-04\n",
      "5     iCaRL  0.000000e+00  0.054332  5.077120e-04\n",
      "6    iCaRL+  0.000000e+00  0.047904  4.598943e-04\n",
      "7      IL2M  1.343821e-08  0.029288  5.545313e-04\n",
      "8     LUCIR  1.577101e-07  0.040530  4.575556e-04\n",
      "9   LwF-GKD  0.000000e+00  0.023136  6.914722e-07\n",
      "10  OvA-Ens  0.000000e+00  0.029005  1.987118e-02\n",
      "11     SSIL  7.306143e-09  0.021222  4.531115e-04\n",
      "   Approach     Pre_Train     Train    Post_Train\n",
      "0       BiC  5.630484e-06  0.047164  3.058019e-03\n",
      "1      EEIL  0.000000e+00  0.039837  1.210057e-02\n",
      "2       EWC  0.000000e+00  0.042834  5.471422e-04\n",
      "3    FT-Mem  0.000000e+00  0.027143  4.659693e-04\n",
      "4    FZ-Mem  0.000000e+00  0.014674  4.684878e-04\n",
      "5     iCaRL  0.000000e+00  0.054557  5.153812e-04\n",
      "6    iCaRL+  0.000000e+00  0.048259  4.668198e-04\n",
      "7      IL2M  1.363465e-08  0.029413  5.629045e-04\n",
      "8     LUCIR  1.602866e-07  0.040778  4.644361e-04\n",
      "9   LwF-GKD  0.000000e+00  0.023208  7.018466e-07\n",
      "10  OvA-Ens  0.000000e+00  0.029120  1.987636e-02\n",
      "11     SSIL  7.398284e-09  0.021337  4.599340e-04\n",
      "   Approach     Pre_Train     Train    Post_Train\n",
      "0       BiC  1.151440e-05  0.034465  1.861135e-02\n",
      "1      EEIL  0.000000e+00  0.025071  1.195519e-02\n",
      "2       EWC  0.000000e+00  0.032204  1.457042e-02\n",
      "3    FT-Mem  0.000000e+00  0.017549  1.230263e-02\n",
      "4    FZ-Mem  0.000000e+00  0.012805  2.951246e-03\n",
      "5     iCaRL  0.000000e+00  0.046452  1.287352e-02\n",
      "6    iCaRL+  0.000000e+00  0.030001  1.287411e-02\n",
      "7      IL2M  0.000000e+00  0.023405  1.309803e-02\n",
      "8     LUCIR  7.247791e-07  0.035893  1.287343e-02\n",
      "9   LwF-GKD  0.000000e+00  0.018321  6.238206e-07\n",
      "10  OvA-Ens  0.000000e+00  0.020698  1.555954e-02\n",
      "11     SSIL  0.000000e+00  0.021571  1.300526e-02\n"
     ]
    }
   ],
   "source": [
    "#39+1 plot times\n",
    "types = [ 'all', 'post_train', 'pre_train', 'post_train_percentage', 'all_normalized_scratch', 'stacked'] \n",
    "for t in types:\n",
    "    plt.close()\n",
    "    figsize = (7, 4)\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    plot_train_time(_df_ti_39, t, appr_task=1, group = 'Last App', nclasses=1, normalize=False, figsize=figsize)\n",
    "    plt.savefig('ExecTime_Scenario_39+1_%s.pdf' % t, bbox_inches='tight')    \n",
    "    \n",
    "    \n",
    "#39+1 plot times\n",
    "types = [ 'all', 'post_train', 'pre_train', 'post_train_percentage', 'all_normalized_scratch', 'stacked'] \n",
    "for t in types:\n",
    "    plt.close()\n",
    "    figsize = (7, 4)\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    plot_train_time(_df_ti_39_mincpu, t, appr_task=1, group = 'Last App', nclasses=1, normalize=False, figsize=figsize)\n",
    "    plt.savefig('ExecTime_Scenario_39+1_%s_mincpu.pdf' % t, bbox_inches='tight') \n",
    "\n",
    "#39+1 plot times\n",
    "types = [ 'all', 'post_train', 'pre_train', 'post_train_percentage', 'all_normalized_scratch', 'stacked'] \n",
    "for t in types:\n",
    "    plt.close()\n",
    "    figsize = (7, 4)\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    plot_train_time(_df_ti_39_mingpu, t, appr_task=1, group = 'Last App', nclasses=1, normalize=False, figsize=figsize)\n",
    "    plt.savefig('ExecTime_Scenario_39+1_%s_mingpu.pdf' % t, bbox_inches='tight') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "35f72bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1279/1279 [00:11<00:00, 115.02it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 26/26 [00:00<00:00, 40.21it/s]\n"
     ]
    }
   ],
   "source": [
    "#load new experiments (controlled)\n",
    "exp_name = 'timing_base20'\n",
    "results_path = '/media/nas/datasets/MIRAGE_2020/FSCIL_approaches/hf-project/results/'\n",
    "df_stdout_filenames = glob('%s/*%s*/stdout*' % (results_path, exp_name), recursive=True)\n",
    "df_stdout_filenames = discard_duplicates(df_stdout_filenames)\n",
    "df_training, df_timing = get_training_info(df_stdout_filenames)\n",
    "\n",
    "#load new experiments (controlled) with 200 epochs\n",
    "exp_name = 'timing_base20'\n",
    "results_path = '/media/nas/datasets/MIRAGE_2020/FSCIL_approaches/hf-project/results/'\n",
    "df_stdout_filenames = glob('%s/*%s*/timing200epochs/stdout*' % (results_path, exp_name), recursive=True)\n",
    "df_stdout_filenames = discard_duplicates(df_stdout_filenames)\n",
    "df_training200, df_timing200 = get_training_info(df_stdout_filenames)\n",
    "\n",
    "_df_tr_eeil = df_training200[(df_training200.Approach=='eeil') & (df_training200.Task==1)]\n",
    "_df_tr_eeil = _df_tr_eeil.drop(_df_tr_eeil.groupby(['Seed']).tail(40).index, axis=0)\n",
    "df_training200_mod = df_training200[~(df_training200.Approach=='eeil')]\n",
    "df_training200_mod = deepcopy(pd.concat([df_training200_mod, _df_tr_eeil]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "b684acca",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Scratch -----\n",
      "max_epo scratch: 173\n",
      "Time all epoches: 3464.6710000000003 \n",
      "------------------\n",
      "\n",
      "Approccio: lucir \n",
      "\n",
      "Seed: 1  - Num_epochs: 200\n",
      "Min 3 cpu epochs: 15.205  - Min all cpu epochs: 15.178\n",
      "Time 3 epochs: 3041.0  - Time all epochs: 3035.6000000000004 \n",
      "\n",
      "Seed: 2  - Num_epochs: 200\n",
      "Min 3 cpu epochs: 14.1  - Min all cpu epochs: 13.975\n",
      "Time 3 epochs: 2820.0  - Time all epochs: 2795.0 \n",
      "\n",
      "Seed: 3  - Num_epochs: 179\n",
      "Min 3 cpu epochs: 14.603  - Min all cpu epochs: 14.576\n",
      "Time 3 epochs: 2613.937  - Time all epochs: 2609.1040000000003 \n",
      "\n",
      "Seed: 4  - Num_epochs: 200\n",
      "Min 3 cpu epochs: 17.349  - Min all cpu epochs: 17.205\n",
      "Time 3 epochs: 3469.8  - Time all epochs: 3440.9999999999995 \n",
      "\n",
      "Ratio min_3 epochs: [0.8777168164019036, 0.8139300961043631, 0.7544546076669328, 1.0014803714407514]\n",
      "Ratio min_all epochs: [0.876158226856172, 0.8067144037630124, 0.7530596700235029, 0.9931678938635152]\n",
      "Differences: [0.0015585895457316257, 0.007215692341350666, 0.0013949376434299054, 0.008312477577236188]\n",
      "Mean of differences: (across seeds): 0.004620424276937096 \n",
      "------------------\n",
      "\n",
      "Approccio: ewc \n",
      "\n",
      "Seed: 1  - Num_epochs: 185\n",
      "Min 3 cpu epochs: 13.559  - Min all cpu epochs: 13.371\n",
      "Time 3 epochs: 2508.415  - Time all epochs: 2473.635 \n",
      "\n",
      "Seed: 2  - Num_epochs: 200\n",
      "Min 3 cpu epochs: 12.427  - Min all cpu epochs: 12.362\n",
      "Time 3 epochs: 2485.4  - Time all epochs: 2472.4 \n",
      "\n",
      "Seed: 3  - Num_epochs: 168\n",
      "Min 3 cpu epochs: 12.943  - Min all cpu epochs: 12.925\n",
      "Time 3 epochs: 2174.424  - Time all epochs: 2171.4 \n",
      "\n",
      "Seed: 4  - Num_epochs: 200\n",
      "Min 3 cpu epochs: 15.409  - Min all cpu epochs: 15.27\n",
      "Time 3 epochs: 3081.8  - Time all epochs: 3054.0 \n",
      "\n",
      "Ratio min_3 epochs: [0.7239980361771723, 0.7173552698077249, 0.6275989841459694, 0.8894928263029881]\n",
      "Ratio min_all epochs: [0.7139595649918853, 0.7136031097902225, 0.6267261740003596, 0.881468976419406]\n",
      "Differences: [0.010038471185287001, 0.0037521600175024483, 0.0008728101456098036, 0.008023849883582068]\n",
      "Mean of differences: (across seeds): 0.00567182280799533 \n",
      "------------------\n",
      "\n",
      "Approccio: eeil \n",
      "\n",
      "Seed: 1  - Num_epochs: 157\n",
      "Min 3 cpu epochs: 14.957  - Min all cpu epochs: 14.796\n",
      "Time 3 epochs: 2348.2490000000003  - Time all epochs: 2322.9719999999998 \n",
      "\n",
      "Seed: 2  - Num_epochs: 193\n",
      "Min 3 cpu epochs: 13.709  - Min all cpu epochs: 13.661\n",
      "Time 3 epochs: 2645.837  - Time all epochs: 2636.573 \n",
      "\n",
      "Seed: 3  - Num_epochs: 168\n",
      "Min 3 cpu epochs: 14.48  - Min all cpu epochs: 14.337\n",
      "Time 3 epochs: 2432.64  - Time all epochs: 2408.616 \n",
      "\n",
      "Seed: 4  - Num_epochs: 170\n",
      "Min 3 cpu epochs: 16.901  - Min all cpu epochs: 16.758\n",
      "Time 3 epochs: 2873.17  - Time all epochs: 2848.8599999999997 \n",
      "\n",
      "Ratio min_3 epochs: [0.6777696929953811, 0.7636618310944964, 0.7021272726905382, 0.8292764305759479]\n",
      "Ratio min_all epochs: [0.670474050782888, 0.7609879841404854, 0.6951932809781939, 0.8222598913432183]\n",
      "Differences: [0.007295642212493125, 0.002673846954010961, 0.006933991712344323, 0.007016539232729602]\n",
      "Mean of differences: (across seeds): 0.005980005027894503 \n",
      "------------------\n",
      "\n",
      "Approccio: bic \n",
      "\n",
      "Seed: 1  - Num_epochs: 194\n",
      "Min 3 cpu epochs: 15.601  - Min all cpu epochs: 15.3\n",
      "Time 3 epochs: 3026.594  - Time all epochs: 2968.2000000000003 \n",
      "\n",
      "Seed: 2  - Num_epochs: 182\n",
      "Min 3 cpu epochs: 14.11  - Min all cpu epochs: 14.09\n",
      "Time 3 epochs: 2568.02  - Time all epochs: 2564.38 \n",
      "\n",
      "Seed: 3  - Num_epochs: 169\n",
      "Min 3 cpu epochs: 14.894  - Min all cpu epochs: 14.796\n",
      "Time 3 epochs: 2517.0860000000002  - Time all epochs: 2500.524 \n",
      "\n",
      "Seed: 4  - Num_epochs: 167\n",
      "Min 3 cpu epochs: 17.587  - Min all cpu epochs: 17.361\n",
      "Time 3 epochs: 2937.029  - Time all epochs: 2899.2870000000003 \n",
      "\n",
      "Ratio min_3 epochs: [0.8735588458471236, 0.7412016898574207, 0.7265007269088465, 0.8477079064650005]\n",
      "Ratio min_all epochs: [0.8567047203038903, 0.7401510850525201, 0.7217204750465483, 0.8368145200511102]\n",
      "Differences: [0.01685412554323329, 0.0010506048049006012, 0.004780251862298135, 0.01089338641389026]\n",
      "Mean of differences: (across seeds): 0.008394592156080571 \n",
      "------------------\n",
      "\n",
      "Approccio: lwf \n",
      "\n",
      "Seed: 1  - Num_epochs: 200\n",
      "Min 3 cpu epochs: 14.593  - Min all cpu epochs: 14.583\n",
      "Time 3 epochs: 2918.6  - Time all epochs: 2916.6 \n",
      "\n",
      "Seed: 2  - Num_epochs: 178\n",
      "Min 3 cpu epochs: 13.479  - Min all cpu epochs: 13.371\n",
      "Time 3 epochs: 2399.2619999999997  - Time all epochs: 2380.038 \n",
      "\n",
      "Seed: 3  - Num_epochs: 173\n",
      "Min 3 cpu epochs: 14.144  - Min all cpu epochs: 14.074\n",
      "Time 3 epochs: 2446.912  - Time all epochs: 2434.802 \n",
      "\n",
      "Seed: 4  - Num_epochs: 179\n",
      "Min 3 cpu epochs: 16.761  - Min all cpu epochs: 16.7\n",
      "Time 3 epochs: 3000.219  - Time all epochs: 2989.2999999999997 \n",
      "\n",
      "Ratio min_3 epochs: [0.8423887866986504, 0.692493457531754, 0.7062465671343685, 0.8659462904269987]\n",
      "Ratio min_all epochs: [0.8418115313113423, 0.6869448787489489, 0.7027512857642183, 0.8627947646399902]\n",
      "Differences: [0.0005772553873080177, 0.005548578782805014, 0.003495281370150205, 0.0031515257870085023]\n",
      "Mean of differences: (across seeds): 0.0031931603318179347 \n",
      "------------------\n",
      "\n",
      "Approccio: chen2021 \n",
      "\n",
      "Seed: 1  - Num_epochs: 3344\n",
      "Min 3 cpu epochs: 11.05  - Min all cpu epochs: 10.806\n",
      "Time 3 epochs: 36951.200000000004  - Time all epochs: 36135.263999999996 \n",
      "\n",
      "Seed: 2  - Num_epochs: 3331\n",
      "Min 3 cpu epochs: 10.2  - Min all cpu epochs: 9.882\n",
      "Time 3 epochs: 33976.2  - Time all epochs: 32916.941999999995 \n",
      "\n",
      "Seed: 3  - Num_epochs: 3317\n",
      "Min 3 cpu epochs: 10.743  - Min all cpu epochs: 10.494\n",
      "Time 3 epochs: 35634.531  - Time all epochs: 34808.598 \n",
      "\n",
      "Ratio min_3 epochs: [10.665139633748774, 9.806472245128035, 10.285112496973017]\n",
      "Ratio min_all epochs: [10.429637907899478, 9.500741051603455, 10.046725360070262]\n",
      "Differences: [0.2355017258492964, 0.3057311935245792, 0.23838713690275526]\n",
      "Mean of differences: (across seeds): 0.2598733520922103 \n",
      "------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_training200 = deepcopy(df_training200_mod)\n",
    "approach_list = ['lucir', 'ewc', 'eeil', 'bic', 'lwf', 'chen2021']\n",
    "print('----- Scratch -----')\n",
    "seed = 1\n",
    "max_epo = max_epochs[(max_epochs['Approach']=='scratch') & (max_epochs['Seed']==seed)]['Epoch'].values[0]\n",
    "print('max_epo scratch:', max_epo)\n",
    "min_all = df_training200[(df_training200['Approach']=='scratch') & (df_training200['Task']==1) & (df_training200['Seed']==seed)]['Train Time'].min()\n",
    "scratch_all = min_all*max_epo\n",
    "print('Time all epoches:', scratch_all, '\\n------------------\\n')\n",
    "seeds = [1,2,3,4]\n",
    "\n",
    "for approach in approach_list:\n",
    "    ratios_3 = []\n",
    "    ratios_all = []\n",
    "    print ('Approccio:', approach, '\\n')\n",
    "    if approach=='chen2021': seeds = seeds[:-1]\n",
    "    for seed in seeds:\n",
    "        max_epo = max_epochs[(max_epochs['Approach']==approach) & (max_epochs['Seed']==seed)]['Epoch'].values[0]\n",
    "        print ('Seed:', seed, ' - Num_epochs:', max_epo)\n",
    "        min3 = df_training200[(df_training200['Approach']==approach) & (df_training200['Task']==1) & (df_training200['Seed']==seed)]['Train Time'].head(3).min()\n",
    "        min_all = df_training200[(df_training200['Approach']==approach) & (df_training200['Task']==1) & (df_training200['Seed']==seed)]['Train Time'].min()\n",
    "        print(\"Min 3 cpu epochs:\", min3, \" - Min all cpu epochs:\", min_all)\n",
    "        time_3 = min3*max_epo\n",
    "        time_all = min_all*max_epo\n",
    "        print('Time 3 epochs:', time_3, ' - Time all epochs:', time_all, '\\n')\n",
    "        ratios_3.append(time_3/scratch_all)\n",
    "        ratios_all.append(time_all/scratch_all)\n",
    "    print('Ratio min_3 epochs:', ratios_3)    \n",
    "    print('Ratio min_all epochs:', ratios_all)\n",
    "    sub = [a_i - b_i for a_i, b_i in zip(ratios_3, ratios_all)]\n",
    "    print('Differences:', sub)\n",
    "    print('Mean of differences: (across seeds):', np.mean(sub), '\\n------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c51a313",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
