{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45334490",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e2aeedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import functools\n",
    "from copy import deepcopy\n",
    "import math\n",
    "\n",
    "from util.metrics_utils import *\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eaadcba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = '/media/nas/datasets/MIRAGE_2020/FSCIL_approaches/hf-project/final_results/results/images'\n",
    "metrics = {'f1_score': 'F1 Score'}\n",
    "\n",
    "metric_dict = dict()\n",
    "lower_is_better_dict = dict()\n",
    "\n",
    "for metric in metrics:\n",
    "    metric_dict.update({\n",
    "        '%s' % metrics[metric]: 'per_class_metrics_metrics_%s_macro-material.parquet' % metric,\n",
    "        '%s Forgetting' % metrics[metric]: 'per_episode_metrics_metrics_forgetting_%s_per episode-material.parquet' % metric,\n",
    "        '%s Intransigence' % metrics[metric]: 'per_class_metrics_metrics_%s_macro intransigence-material.parquet' % metric,\n",
    "        '%s Drop' % metrics[metric]: 'per_class_metrics_metrics_%s_macro intransigence-material.parquet' % metric,\n",
    "        '%s Drop Old' % metrics[metric]: 'per_class_metrics_metrics_%s_macro intransigence-material.parquet' % metric,\n",
    "    })\n",
    "\n",
    "    lower_is_better_dict.update({\n",
    "        '%s' % metrics[metric]: False,\n",
    "        '%s Forgetting' % metrics[metric]: True,\n",
    "        '%s Intransigence' % metrics[metric]: True,\n",
    "        '%s Drop' % metrics[metric]: True,\n",
    "        '%s Drop Old' % metrics[metric]: True\n",
    "    })\n",
    "\n",
    "scenario_discrs_dict = {\n",
    "   '$CV_{5}$': ['coervionba20inr5stp5'],\n",
    "   '$CV_{10}$': ['coervionba20in10stp3'],\n",
    "   '$CV_{20}$': ['coervionba20in20stp2'],\n",
    "   '$TC_{1}$': ['tricclonba34inr1stp7'],\n",
    "   '$39_{1}$': ['393911ba39inr1stp2'],\n",
    "   '$38_{2}$': ['tricclonba38inr2stp2'],\n",
    "   '$TC_{2}$': ['tricclonba34inr2stp4'],\n",
    "   '$TC_{3}$': ['tricclonba34inr3stp3'],\n",
    "#    '$CV$': ['coervionba20inr5stp5', 'coervionba20in10stp3', 'coervionba20in20stp2'],\n",
    "#    '$TC$': ['tricclonba34inr1stp7', 'tricclonba34inr2stp4', 'tricclonba34inr3stp3'],\n",
    "}\n",
    "\n",
    "scenario_descr_dict = {\n",
    "    '$CV_{5}$': 'Computer Vision (CV) - Base: 20; Incr: 5 [Rank @ 40 Apps]',\n",
    "    '$CV_{10}$': 'Computer Vision (CV) - Base: 20; Incr: 10 [Rank @ 40 Apps]',\n",
    "    '$CV_{20}$': 'Computer Vision (CV) - Base: 20; Incr: 20 [Rank @ 40 Apps]',\n",
    "    '$TC_{1}$': 'Traffic Classification (TC) - Base: 34; Incr: 1 [Rank @ 40 Apps]',\n",
    "    '$39_{1}$': 'Traffic Classification (TC) - Base: 39; Incr: 1 [Rank @ 40 Apps]',\n",
    "    '$38_{2}$': 'Traffic Classification (TC) - Base: 38; Incr: 2 [Rank @ 40 Apps]',\n",
    "    '$TC_{2}$': 'Traffic Classification (TC) - Base: 34; Incr: 2 [Rank @ 40 Apps]',\n",
    "    '$TC_{3}$': 'Traffic Classification (TC) - Base: 34; Incr: 3 [Rank @ 40 Apps]',\n",
    "#     '$CV$': 'Computer Vision (CV) - Base: 20; Incr: 5, 10, and 20 [Rank @ 40 Apps]',\n",
    "#     '$TC$': 'Traffic Classification (TC) - Base: 34; Incr: 1, 2, and 3 [Rank @ 40 Apps]',\n",
    "}\n",
    "\n",
    "selected_approaches_dict = {\n",
    "#     'incr': ['icarl', 'icarlp', 'bic', 'il2m', 'lwf', 'lucir', 'ewc', 'eeil', 'ssil', 'wu2022', 'chen2021', 'joint'],\n",
    "    'incr': ['icarl', 'icarlp', 'bic', 'il2m', 'lwfgkd', 'lucir', 'ewc', 'eeil', 'ssil', 'chen2021', 'scratch'],\n",
    "#     'naive': ['freezing', 'finetuning', 'backbonefreezing', 'jointft', 'jointmem', 'joint']\n",
    "    'naive': ['freezing', 'finetuning', 'backbonefreezing', 'jointft', 'jointmem', 'backbonefreezingmem', 'scratch'],\n",
    "    'kd': ['lwf', 'lwfgkd', 'scratch']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46480820",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:06<00:00,  1.29it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:04<00:00,  1.68it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:03<00:00,  2.45it/s]\n"
     ]
    }
   ],
   "source": [
    "df_rank = {}\n",
    "for analysis in selected_approaches_dict:\n",
    "    rank_rows = []\n",
    "    for scenario_discrs in tqdm(scenario_discrs_dict):\n",
    "        tmp_rank_rows = []\n",
    "        for metric in metric_dict:\n",
    "            files = []\n",
    "            for scenario_discr in scenario_discrs_dict[scenario_discrs]:\n",
    "                files.extend(glob('%s/%s_*/material/%s' % (img_path, scenario_discr, metric_dict[metric])))\n",
    "\n",
    "            if len(files)==0:\n",
    "                print('No files found for %s %s' % (scenario_discr, metric_dict[metric]))\n",
    "                continue\n",
    "\n",
    "            dfs = []\n",
    "            for file in files:\n",
    "                dfs.append(pd.read_parquet(file))\n",
    "            df = pd.concat(dfs, axis=0)\n",
    "            df.loc[:, 'Approach'] = df.loc[:, 'Approach'].apply(lambda x: appr_dict_r[x])\n",
    "\n",
    "#             df.loc[(df['Approach'] == 'joint') & ~(df['Memory Size'] == 0), 'Approach'] = 'jointmem'\n",
    "#             df.loc[(df['Approach'] == 'backbonefreezing') & ~(df['Memory Size'] == 0), 'Approach'] = 'backbonefreezingmem'\n",
    "            df = df[df['Approach'].isin(selected_approaches_dict[analysis])]\n",
    "\n",
    "            if 'Forgetting' in metric:\n",
    "                df_40 = df[(df['#Apps'] == 40) & (df['Type'] == 'Old')]\n",
    "            elif 'Intransigence' in metric:\n",
    "                df_40 = df[(df['#Apps'] == 40) & (df['Type'] == 'New')]\n",
    "                df_40.rename(columns={'F1 Score Drop': 'F1 Score Intransigence'}, inplace=True)\n",
    "            elif 'Drop Old' in metric:\n",
    "                df_40 = df[(df['#Apps'] == 40) & (df['Type'] == 'Old')]\n",
    "                df_40.rename(columns={'F1 Score Drop': 'F1 Score Drop Old'}, inplace=True)\n",
    "            else:\n",
    "                if 'Drop' not in metric:\n",
    "                    complement = False\n",
    "                df_40 = df[(df['#Apps'] == 40) & (df['Type'] == 'All')]\n",
    "\n",
    "            if 'Last App' not in df_40:\n",
    "                df_40.loc[:, 'Last App'] = -1\n",
    "            \n",
    "            complement = False\n",
    "            for g, d in df_40.groupby(['Seed', 'Increment', 'Last App']):\n",
    "                if g[0] == 0 and g[2] == -1:\n",
    "                    continue\n",
    "                \n",
    "                tmp_d = d.drop_duplicates('Approach')\n",
    "                row = {'Scenario': scenario_discrs, 'Seed': g[0], 'Increment': g[1], 'Last App': g[2],\n",
    "                       'Metric': ('c' if complement else '') + metric}\n",
    "                r = 0\n",
    "                for appr in tmp_d.sort_values(metric, ascending=lower_is_better_dict[metric])['Approach'].values:\n",
    "                    value = float(tmp_d[tmp_d['Approach'] == appr][metric].values[0])\n",
    "                    if appr != 'scratch':\n",
    "                        r += 1\n",
    "                        rank = r\n",
    "                    else:\n",
    "                        rank = np.nan\n",
    "                    row.update({'Approach': appr, 'Rank': rank, 'Value [%]': 100 - value if complement else value})\n",
    "                    tmp_rank_rows.append(deepcopy(row))\n",
    "\n",
    "        tmp_df = pd.DataFrame(tmp_rank_rows)\n",
    "        for g, d in tmp_df.groupby(['Seed', 'Increment', 'Last App']):\n",
    "            tmp_d = d.groupby(['Approach'])['Value [%]'].mean().reset_index()\n",
    "            row = {'Scenario': scenario_discrs, 'Seed': g[0], 'Increment': g[1], 'Last App': g[2], 'Metric': 'Average'}\n",
    "            r = 0\n",
    "            for appr in tmp_d.sort_values('Value [%]', ascending=False)['Approach'].values:\n",
    "                value = float(tmp_d[tmp_d['Approach'] == appr]['Value [%]'].values[0])\n",
    "                if appr != 'scratch':\n",
    "                    r += 1\n",
    "                    rank = r\n",
    "                else:\n",
    "                    rank = np.nan\n",
    "                row.update({'Approach': appr, 'Rank': rank, 'Value [%]': value})\n",
    "                tmp_rank_rows.append(deepcopy(row))\n",
    "\n",
    "        rank_rows.extend(tmp_rank_rows)\n",
    "\n",
    "    df_rank[analysis] = pd.DataFrame(rank_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb39b802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['F1 Score', 'F1 Score Forgetting', 'F1 Score Intransigence',\n",
       "       'F1 Score Drop', 'F1 Score Drop Old', 'Average'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rank['incr']['Metric'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "608cd2b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Scenario', 'Seed', 'Increment', 'Last App', 'Metric', 'Approach',\n",
       "       'Rank', 'Value [%]'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rank['incr'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c5afbce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_scenario = ['$39_{1}$', '$CV_{20}$']\n",
    "csv_path = os.path.join(img_path, 'radarplot_final')\n",
    "for approach in selected_approaches_dict.keys():\n",
    "    if approach != 'kd':\n",
    "        for scenario in selected_scenario:\n",
    "            stats_df = df_rank[approach][(df_rank[approach]['Scenario'] == scenario)].groupby(['Approach', 'Metric']).describe().reset_index()[['Approach', 'Metric', 'Value [%]']]\n",
    "            df = pd.concat([stats_df['Approach'], stats_df['Metric'], stats_df['Value [%]']['mean']], axis=1)\n",
    "            #print(df)\n",
    "            #input('check')\n",
    "            df.to_csv(os.path.join(csv_path, scenario + '_' + approach + '.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2fd9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_scenario = ['$39_{1}$', '$CV_{20}$']\n",
    "df_rank['naive'][(df_rank['naive']['Scenario'] == selected_scenario[0]) & (df_rank['naive']['Metric'] == 'F1 Score')].groupby('Approach').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e059ebe9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_df_rank = deepcopy(df_rank)\n",
    "pd.concat(_df_rank.values()).to_parquet('data/ranking_data.parquet')\n",
    "\n",
    "for analysis in _df_rank:\n",
    "    df_groups = _df_rank[analysis].groupby(['Scenario', 'Metric'])\n",
    "    df_list = []\n",
    "    for gr in tqdm(df_groups.groups):\n",
    "        gr_df = df_groups.get_group(gr)\n",
    "        if 'scratch' not in gr_df['Approach'].unique():\n",
    "            g_df = gr_df.groupby(['Seed', 'Increment', 'Last App'])\n",
    "            for g in g_df.groups:\n",
    "                sel_df = g_df.get_group(g)\n",
    "                sel_df.loc[len(sel_df.index)] = [gr[0], g[0], g[1], g[2], gr[1], 'scratch', np.nan, np.nan]\n",
    "                df_list.append(sel_df)\n",
    "        else:\n",
    "            df_list.append(gr_df)\n",
    "    _df_rank[analysis] = pd.concat(df_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544d53cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "_df_rank['kd']['Approach'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f893fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "_df_rank['kd'][_df_rank['kd']['Scenario'] == '$39_{1}$'].groupby(['Metric', 'Approach']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6c07bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _df_rank = deepcopy(df_rank)\n",
    "# for key in _df_rank:\n",
    "#     _df_rank[key].loc[:, ['Metric', 'Value [%]']] = _df_rank[key].loc[:, ['Metric', 'Value [%]']].apply(\n",
    "#         lambda x: x if x['Metric'] == 'F1 Score' else {'Metric': 'c' + x['Metric'], 'Value [%]': 100 - x['Value [%]']},\n",
    "#         axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1854b3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_rank_per_dataset(ser, ascending=True):\n",
    "#     ser_sorted = ser.sort_values(ascending=ascending)\n",
    "#     ser_ranked = pd.Series(np.arange(1, ser.shape[0]+1), index=ser_sorted.index.tolist())\n",
    "#     ser_ranked = ser_ranked.loc[ser.index]\n",
    "#     return ser_ranked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ca532e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# __df = _df_rank['incr'][(_df_rank['incr']['Scenario'].isin(['$39_{1}$']))].groupby(['Approach', 'Scenario', 'Last App'])['Value [%]'].mean().reset_index()\n",
    "# __df_m = __df.pivot(index='Last App', columns='Approach', values='Value [%]')\n",
    "# __df_r = __df_m.apply(compute_rank_per_dataset, axis=1, ascending=False)\n",
    "# ser_avg_rank = __df_r.mean(axis=0)\n",
    "\n",
    "# print(ser_avg_rank.sort_values())\n",
    "\n",
    "# __r = []\n",
    "# __df_g = __df.groupby('Last App')\n",
    "# for g, d in __df_g:\n",
    "#     for r, appr in enumerate(d.sort_values('Value [%]', ascending=False)['Approach'].values):\n",
    "#         __r.append({'Dataset': g, 'Approach': appr, 'Rank': r + 1})\n",
    "\n",
    "# print(pd.DataFrame(__r).groupby('Approach')['Rank'].mean().sort_values())\n",
    "\n",
    "# print(pd.DataFrame(_df_rank['incr'][\n",
    "#     (_df_rank['incr']['Scenario'].isin(['$39_{1}$'])) &\n",
    "#     (_df_rank['incr']['Metric'] == 'Average')\n",
    "# ]).groupby('Approach')['Rank'].mean().sort_values())\n",
    "\n",
    "# print(__df.groupby('Approach')['Value [%]'].mean().sort_values(ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020cab90",
   "metadata": {},
   "outputs": [],
   "source": [
    "_df_rank['incr']['Approach'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad303183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _df_rank['incr'][(_df_rank['incr']['Approach'] == 'joint') & (_df_rank['incr']['Scenario'] == '$39_{1}$')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92a9642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_mean_rank_per_appr = _df_rank.groupby(['Scenario', 'Metric', 'Approach', 'Increment'])['Rank'].mean().reset_index()\n",
    "# df_mean_rank_per_appr = df_mean_rank_per_appr.groupby(['Scenario', 'Approach', 'Increment'])['Rank'].mean().reset_index()\n",
    "# df_std_rank_per_appr = _df_rank.groupby(['Scenario', 'Approach', 'Increment'])['Rank'].std().reset_index()\n",
    "# df_mean_rank_per_appr.loc[:, 'StdDev'] = df_std_rank_per_appr['Rank']\n",
    "\n",
    "# df_mean_rank_per_appr.loc[df_mean_rank_per_appr['Approach'].isin(['wu2022', 'chen2021', 'icarlp']), 'Type'] = 'Net'\n",
    "# df_mean_rank_per_appr.loc[~df_mean_rank_per_appr['Approach'].isin(['wu2022', 'chen2021', 'icarlp']), 'Type'] = 'Vis'\n",
    "\n",
    "# cv20_filt = df_mean_rank_per_appr['Scenario'] == '$CV_{20}$'\n",
    "# tc1_filt = df_mean_rank_per_appr['Scenario'] == '$39_{1}$'\n",
    "\n",
    "# print(df_mean_rank_per_appr.loc[cv20_filt, 'Scenario'].unique())\n",
    "# print(df_mean_rank_per_appr.loc[tc1_filt, 'Scenario'].unique())\n",
    "# print(df_mean_rank_per_appr[tc1_filt])\n",
    "# print(df_mean_rank_per_appr[cv20_filt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078500ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric = 'F1 Score'\n",
    "# discr = 'c'\n",
    "# r = 'Value [%]'\n",
    "# metrics_list = ['%s' % metric,\n",
    "#         discr + '%s Intransigence' % metric,\n",
    "#         discr + '%s Forgetting' % metric,\n",
    "#         discr + '%s Drop' % metric,\n",
    "#         discr + '%s Drop Old' % metric]\n",
    "# order_metrics_list = [2, 3, 0, 1, 4]\n",
    "# sorting_per_scenario = {}\n",
    "# for analysis in selected_approaches_dict:\n",
    "#     sorting_per_scenario[analysis]={}\n",
    "#     df = _df_rank[analysis]\n",
    "#     for scenario in df['Scenario'].unique():\n",
    "#         approaches = df['Approach'].unique()\n",
    "#         approaches_values_arr = np.zeros(approaches.shape[0])\n",
    "#         for i in range(approaches.shape[0]):\n",
    "#             df_g = df[(df['Approach'] == approach) & (df['Scenario'] == scenario)].groupby(['Metric'])\n",
    "#             appr_df_tmp_list = []\n",
    "#             for g, d in df_g:\n",
    "#                 fact = 0\n",
    "#                 appr_df_tmp_list.append([g, abs(fact - d[r].mean()), d[r].std()])\n",
    "#             appr_df_tmp = pd.DataFrame(columns=['Metric', 'Value', 'Standard'], data=appr_df_tmp_list)\n",
    "#             appr_metrics_values = list(appr_df_tmp[appr_df_tmp['Metric'].isin(metrics_list)]['Value'].values)\n",
    "#             #print(appr_metrics_values)\n",
    "#             appr_metrics_values = [appr_metrics_values[i] for i in order_metrics_list]\n",
    "#             #print(appr_metrics_values)\n",
    "#             approaches_values_arr = sum([appr_metrics_values[i] * appr_metrics_values[(i+1) % len(order_metrics_list)] for i in range(len(appr_metrics_values))])\n",
    "        \n",
    "# #         print(approaches)\n",
    "# #         print([approaches_values_arr[i] for i in np.argsort(approaches_values_arr)])\n",
    "# #         input()\n",
    "#         sorting_per_scenario[analysis][scenario] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b052344",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot_spasa_di_ranking_spider(r='Rank', metric='acc', r_range=(12, 1), autoticks=True,savefig=os.path.join(ranking_path, 'rank'), plot_std=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a479a69d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#plot_spasa_di_ranking_spider(r='Rank', metric='f1', r_range=(12, 1), autoticks=True, plot_std=False)\n",
    "#plot_spasa_di_ranking_spider(r='Rank', metric='f1', r_range=(12, 1), autoticks=True,savefig=os.path.join(ranking_path, 'rank'), plot_std=True, top=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f10960",
   "metadata": {},
   "outputs": [],
   "source": [
    "_df_rank['incr']['Metric'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519fbb9a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.io import write_image\n",
    "from copy import copy\n",
    "from util.colour_mapper import *\n",
    "import plotly.io as pio\n",
    "import math\n",
    "palette = colour_mapper()\n",
    "ranking_by_area = False\n",
    "\n",
    "linestyles = {\n",
    "    'incr': {\n",
    "        '$38_{2}$': {\n",
    "            'chen2021': 'solid', 'bic': 'dash', 'lwfgkd': 'dot',\n",
    "            'ewc': 'solid', 'lucir': 'dash', 'icarlp': 'dot',\n",
    "            'eeil': 'dash', 'il2m': 'dot'\n",
    "        },\n",
    "        '$39_{1}$': {\n",
    "            'bic': 'solid', 'chen2021': 'dash', 'lwfgkd': 'dot',\n",
    "            'icarlp': 'solid', 'ewc': 'dash', 'eeil': 'dot',\n",
    "            'lucir': 'solid', 'il2m': 'dash'\n",
    "        },\n",
    "        '$CV_{20}$': {\n",
    "            'chen2021': 'solid', 'bic': 'dash', 'lucir': 'dot',\n",
    "            'lwf': 'solid', 'ssil': 'dash',\n",
    "            'eeil': 'solid', 'icarlp': 'dash', 'il2m': 'dot'\n",
    "        }\n",
    "    },\n",
    "    'naive': {\n",
    "        '$38_{2}$': {\n",
    "            'jointmem': 'solid', 'backbonefreezingmem': 'dash',\n",
    "            'finetuning': 'solid', 'jointft': 'dash',\n",
    "            'backbonefreezing': 'solid', 'freezing': 'dash',\n",
    "        },\n",
    "        '$39_{1}$': {\n",
    "            'backbonefreezingmem': 'solid', 'jointmem': 'dash',\n",
    "            'finetuning': 'solid', 'jointft': 'dash',\n",
    "            'backbonefreezing': 'solid', 'freezing': 'dash',\n",
    "        },\n",
    "        '$CV_{20}$': {\n",
    "            'jointmem': 'solid', 'backbonefreezingmem': 'dash',\n",
    "            'finetuning': 'dash', 'jointft': 'dot',\n",
    "            'freezing': 'dash', 'backbonefreezing': 'dot'\n",
    "        }\n",
    "    },\n",
    "    'kd': {\n",
    "        '$39_{1}$': {\n",
    "            'lwf': 'solid', 'lwfgkd': 'dash'\n",
    "        },\n",
    "        '$CV_{20}$': {\n",
    "            'lwf': 'solid', 'lwfgkd': 'dash'\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def plot_spasa_di_ranking_spider(data, r='Rank', metric='f1', r_range=(12, 1), autoticks=False, plot_std=False, savefig=None,\n",
    "                                width=850, height=580, dpi=300, top=None, fill=None, show_annotations=False, best_mode='max',\n",
    "                                show_title=True, axis_angle=0, selected_approaches=None, legend=True, sorted_approaches=None,\n",
    "                                order_metrics = [2, 3, 0, 1, 4], dashes={}, selected_scenario=None, min_max_norm=True, fig_format='pdf'):\n",
    "    \n",
    "    def to_plotly_color(mpl_color, alpha=1):\n",
    "        plotly_color = 'rgba(%d,%d,%d,%.1f)' % (tuple(v * 255 for v in mpl_color[:3]) + (alpha,))\n",
    "        return plotly_color\n",
    "    \n",
    "    colors_dict = dict([(k, v) for k, v in zip(\n",
    "        data['Approach'].unique(), [cm.get_cmap('tab10')(i/10) for i in range(10)] + [(.1, .1, .1, 1.0)])])\n",
    "    colors_dict = palette\n",
    "    markers_dict = dict([(k, v) for v, k in enumerate(data['Approach'].unique())])\n",
    "    \n",
    "    discr = '' if r == 'Rank' or r == 'Value [%]' else 'c'\n",
    "    \n",
    "    metrics_dict = {\n",
    "        'F1 Score': 'Overall',\n",
    "        discr + 'F1 Score Intransigence': discr + 'DropNew',\n",
    "#        discr + 'F1 Score Forgetting': discr + 'Forgetting',\n",
    "        discr + 'F1 Score Drop': discr + 'DropAverage',\n",
    "        discr + 'F1 Score Drop Old': discr + 'DropOld',\n",
    "#         discr + 'F1 Score': discr + 'Average',\n",
    "#         'F1 Score Intransigence': 'Intransigence',\n",
    "#         'F1 Score Forgetting': 'Forgetting',\n",
    "#         'F1 Score Drop': 'Drop-Average',\n",
    "#         'F1 Score Drop Old': 'Drop-Old',\n",
    "        #'Accuracy All': 'Accuracy Average',\n",
    "        #'Accuracy Intransigence': discr + 'Accuracy Intransigence',\n",
    "        #'Accuracy Forgetting Per Episode': discr + 'Accuracy Forgetting',\n",
    "        #'Accuracy Drop All': discr + 'Accuracy Average Drop',\n",
    "        #'Accuracy Drop Old': discr + 'Accuracy Old Drop'\n",
    "    }\n",
    "    # Fix LwF-GKD label (if exiists) to LwF\n",
    "    for appr in list(appr_dict.keys()):\n",
    "        if 'lwf' in appr:\n",
    "            appr_dict[appr] = 'LwF'\n",
    "\n",
    "    r_metrics_dict = dict([(metrics_dict[k], k) for k in metrics_dict])\n",
    "    metrics = [m for i, m in enumerate(list(metrics_dict.values())) if metric in list(metrics_dict.keys())[i].lower()]\n",
    "    for scenario in data['Scenario'].unique() if not selected_scenario else selected_scenario:\n",
    "                \n",
    "        fig = go.Figure()\n",
    "        \n",
    "        df_g0 = data[(data['Scenario'] == scenario) & (data['Metric'] == 'Average')].groupby('Approach')\n",
    "#         approaches = list(df_g0['Value [%]'].mean().sort_values(ascending=False).index)\n",
    "        if sorted_approaches:\n",
    "            approaches = sorted_approaches[scenario]\n",
    "        else:\n",
    "            approaches = list(df_g0['Rank'].mean().sort_values(ascending=True).index)\n",
    "        approaches = [a for a in approaches if a in selected_approaches]\n",
    "        if show_annotations:\n",
    "            best_by_metric={}\n",
    "            worst_by_metric={}\n",
    "            best_by_value={}\n",
    "            worst_by_value={}\n",
    "            for met, dfmet in data[\n",
    "                (data['Scenario'] == scenario) &\n",
    "                (data['Metric'].isin([r_metrics_dict[m] for m in metrics])) &\n",
    "                (data['Approach'].isin(approaches))].groupby(['Metric']):\n",
    "                if top:\n",
    "                    dfmet=dfmet[dfmet['Approach'].isin(approaches[:top])]\n",
    "                # Exclude scratch from list of ordered approch by mean (:-1)\n",
    "                ordered_appr=list(dfmet.groupby('Approach')[r].mean().sort_values().index)[:-1]\n",
    "                ordered_appr_values=list(dfmet.groupby('Approach')[r].mean().sort_values())[:-1]\n",
    "                ordered_appr=[a for a in ordered_appr if a in appr_dict.keys()]\n",
    "                best_by_metric[metrics_dict[met]]=ordered_appr[-1] if best_mode == 'max' else ordered_appr[0]\n",
    "                worst_by_metric[metrics_dict[met]]=ordered_appr[0] if best_mode == 'max' else ordered_appr[-1]\n",
    "                best_by_value[metrics_dict[met]]=round(ordered_appr_values[-1], 1) if best_mode == 'max' else round(ordered_appr_values[0], 1)\n",
    "                worst_by_value[metrics_dict[met]]=round(ordered_appr_values[0], 1) if best_mode == 'max' else round(ordered_appr_values[-1], 1)\n",
    "                    \n",
    "            print('Best per metric:', best_by_metric)\n",
    "            print('Worst per metric:', worst_by_metric)\n",
    "            \n",
    "            best_by_value = {item[0]: '{}%'.format(item[1]) for item in best_by_value.items()}\n",
    "            worst_by_value = {item[0]: '{}%'.format(item[1]) for item in worst_by_value.items()}\n",
    "            print('Best per value:', best_by_value)\n",
    "            print('Worst per value:', worst_by_value)\n",
    "        if min_max_norm:\n",
    "            metric_avg_vals_dict = {metric: [] for metric in metrics_dict}\n",
    "            for approach in approaches[:top] if top is not None and top < len(approaches) else approaches:\n",
    "                df_g = data[(data['Approach'] == approach) & (data['Scenario'] == scenario)].groupby(['Metric'])\n",
    "                for metric in metrics_dict:\n",
    "                    metric_avg_vals_dict[metric].append(df_g.get_group(metric)[r].mean())\n",
    "            min_max_dict = {metrics_dict[metric]: (min((val for val in metric_avg_vals_dict[metric] if not math.isnan(val))),\n",
    "                                max((val for val in metric_avg_vals_dict[metric] if not math.isnan(val))))\n",
    "                        for metric in metrics_dict}\n",
    "\n",
    "        for approach in approaches[:top] if top is not None and top < len(approaches) else approaches:\n",
    "            df_tmp = pd.DataFrame()\n",
    "            df_g = data[(data['Approach'] == approach) & (data['Scenario'] == scenario)].groupby(['Metric'])\n",
    "            \n",
    "\n",
    "            for g, d in df_g:\n",
    "                fact = 0\n",
    "                df_tmp = df_tmp.append(\n",
    "                    {'Metric': g, 'Value': abs(fact - d[r].mean()), 'Standard': d[r].std()}, ignore_index=True\n",
    "                )\n",
    "\n",
    "\n",
    "            df_tmp=df_tmp[df_tmp['Metric'].isin(metrics_dict.keys())]\n",
    "            df_tmp.loc[:, 'Metric'] = df_tmp['Metric'].apply(lambda x: metrics_dict[x])\n",
    "            \n",
    "            if approach == 'scratch':\n",
    "                print('Scratch Overall: %s' % df_tmp[df_tmp['Metric'] == 'Overall']['Value'].values[0])\n",
    "            \n",
    "            if min_max_norm:\n",
    "                for key in min_max_dict:\n",
    "                    df_tmp['Value'].loc[df_tmp['Metric'] == key] = (df_tmp['Value'] - min_max_dict[key][0]) / (min_max_dict[key][1] - min_max_dict[key][0])\n",
    "            \n",
    "            #Masking overall metric in plot\n",
    "            if 'Overall' in df_tmp['Metric'].unique():\n",
    "                df_tmp=df_tmp[df_tmp['Metric'] != 'Overall']\n",
    "\n",
    "            radious = list(df_tmp[(df_tmp['Metric'].isin(metrics))]['Value'].values)\n",
    "            radious = [radious[i] for i in order_metrics]\n",
    "            radious += [radious[0]]\n",
    "            \n",
    "            thetas = list(df_tmp[(df_tmp['Metric'].isin(metrics))]['Metric'].values)\n",
    "            thetas = [thetas[i] for i in order_metrics]\n",
    "            \n",
    "            if show_annotations:\n",
    "                thetas=['<b>%s</b><br>&#9786;%s: %s<br>&#9785;%s: %s' % ('<br>'.join(t.split('-')), appr_dict[best_by_metric[t]], best_by_value[t], appr_dict[worst_by_metric[t]], worst_by_value[t]) for t in thetas]\n",
    "            thetas += [thetas[0]]\n",
    "            if not all([True if math.isnan(val) else False for val in radious]):\n",
    "                fig.add_trace(\n",
    "                    go.Scatterpolar(\n",
    "                        r=radious,\n",
    "                        theta=thetas,\n",
    "                        fill='toself' if fill and plot_std else 'none',\n",
    "                        line={'color': to_plotly_color(colors_dict[approach], 1.), 'width':5 if approach != 'scratch' else 0,\n",
    "                              'dash': dashes.get(scenario, {}).get(approach, 'solid')},\n",
    "                        fillcolor=to_plotly_color(colors_dict[approach], .15),\n",
    "                        marker_symbol=markers_dict[approach] if approach != 'scratch' else 'star',\n",
    "                        marker_size=15,\n",
    "                        name=appr_dict[approach]\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                if plot_std:\n",
    "                    radious_std=list(df_tmp[(df_tmp['Metric'].isin(metrics))]['Standard'].values)\n",
    "                    radious_std = [radious_std[i] for i in [2, 3, 0, 1, 4]]\n",
    "\n",
    "                    radious_std += [radious_std[0]]\n",
    "\n",
    "                    radious_std_p=[m+d for (m,d) in zip(radious,radious_std)]\n",
    "                    radious_std_n=[m-d for (m,d) in zip(radious,radious_std)]\n",
    "\n",
    "                    fig.add_trace(\n",
    "                        go.Scatterpolar(\n",
    "                            r=radious_std_p,\n",
    "                            theta=thetas,\n",
    "                            fill='none',\n",
    "                            line={'color': to_plotly_color(colors_dict[approach], .7), 'dash':'dot', 'width':3},\n",
    "                            fillcolor=to_plotly_color(colors_dict[approach], .15),\n",
    "                            marker_symbol=markers_dict[approach],\n",
    "                            marker_size=0,\n",
    "                            showlegend=False\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                    fig.add_trace(\n",
    "                        go.Scatterpolar(\n",
    "                            r=radious_std_n,\n",
    "                            theta=thetas,\n",
    "                            fill='none',\n",
    "                            line={'color': to_plotly_color(colors_dict[approach], .7), 'dash':'dot', 'width':3},\n",
    "                            fillcolor=to_plotly_color([256,256,256,256], .3),\n",
    "                            marker_symbol=markers_dict[approach],\n",
    "                            marker_size=0,\n",
    "                            showlegend=False\n",
    "                        )\n",
    "                    )\n",
    "        \n",
    "        title_text = 'Scenario %s' % scenario_descr_dict[scenario]\n",
    "        if not show_title:\n",
    "            print(title_text)\n",
    "            title_text = None\n",
    "\n",
    "        update_layout_dict = dict(\n",
    "            title_text=title_text,\n",
    "            font=dict(\n",
    "                family='Arial',\n",
    "                size=18,\n",
    "                color='black'\n",
    "            ),\n",
    "            #legend_title_text='Average Rank' if not sorted_approaches else 'Area Ranking',\n",
    "            width=width,\n",
    "            height=height,\n",
    "            showlegend=legend,\n",
    "            # annotations=[dict(align='center')]\n",
    "        )\n",
    "        \n",
    "        update_polars_dict = dict(\n",
    "            radialaxis=dict(\n",
    "                # showticklabels=False,\n",
    "                range=r_range,\n",
    "                angle = axis_angle,\n",
    "                gridcolor='#bbbbbb',\n",
    "                tickfont=dict(\n",
    "                    family='Arial Black',\n",
    "                    size=18,\n",
    "                    color='black'\n",
    "                ),\n",
    "                dtick=10 if not min_max_norm else 0.1\n",
    "            ),\n",
    "            bgcolor='rgba(250, 250, 250, 200)',\n",
    "            angularaxis_rotation=-20\n",
    "        )\n",
    "\n",
    "        fig.update_layout(**update_layout_dict)\n",
    "        fig.update_polars(**update_polars_dict)\n",
    "\n",
    "        fig.show()\n",
    "        \n",
    "        # How to suppress textbox \"\"Loading [MathJax]/extensions/MathMenu.js\"\" into pdf images  \n",
    "        # https://github.com/plotly/plotly.py/issues/3469\n",
    "        pio.full_figure_for_development(fig, warn=False)\n",
    "        if savefig is not None:\n",
    "            write_image(fig, '%s.pdf' % '_'.join(\n",
    "                [savefig] + scenario_discrs_dict[scenario] + [metric] +\n",
    "                ['%s'%('Top%s'%(top if top is not None and top<len(approaches) else ''))]), format=fig_format, engine='kaleido')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83943eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_approaches_by_area(df, metrics_list, order_metrics_list):\n",
    "    sorting_per_scenario={}\n",
    "    for scenario in df['Scenario'].unique():\n",
    "        approaches = df['Approach'].unique()\n",
    "        approaches_values_arr = np.zeros(approaches.shape[0])\n",
    "        for i in range(approaches.shape[0]):\n",
    "            df_g = df[(df['Approach'] == approaches[i]) & (df['Scenario'] == scenario)].groupby(['Metric'])\n",
    "            appr_df_tmp_list = []\n",
    "            for g, d in df_g:\n",
    "                fact = 0\n",
    "                appr_df_tmp_list.append([g, abs(fact - d[r].mean()), d[r].std()])\n",
    "            appr_df_tmp = pd.DataFrame(columns=['Metric', 'Value', 'Standard'], data=appr_df_tmp_list)\n",
    "            appr_metrics_values = list(appr_df_tmp[appr_df_tmp['Metric'].isin(metrics_list)]['Value'].values)\n",
    "            appr_metrics_values = [appr_metrics_values[i] for i in order_metrics_list]\n",
    "            approaches_values_arr[i] = sum([appr_metrics_values[i] * appr_metrics_values[(i+1) % len(order_metrics_list)] for i in range(len(appr_metrics_values))])\n",
    "        indexes = np.argsort(approaches_values_arr)\n",
    "        if np.any(np.isnan(approaches_values_arr)):\n",
    "            indexes_temp = np.zeros(indexes.shape[0], dtype=np.int16)\n",
    "            indexes_nonan = indexes[:-1]\n",
    "            indexes_temp[:-1] = indexes_nonan\n",
    "            indexes_temp[-1] = indexes[-1]\n",
    "            indexes = indexes_temp\n",
    "        \n",
    "        sorting_per_scenario[scenario] = [approaches[i] for i in indexes]\n",
    "    return sorting_per_scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36918a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# metric = 'F1 Score'\n",
    "# discr = 'c'\n",
    "# r = 'Value [%]'\n",
    "# metrics_list = ['%s' % metric,\n",
    "#         discr + '%s Intransigence' % metric,\n",
    "#         discr + '%s Forgetting' % metric,\n",
    "#         discr + '%s Drop' % metric,\n",
    "#         discr + '%s Drop Old' % metric]\n",
    "# analysis_list = ['incr', 'naive']\n",
    "# selected_scenario = ['$38_{2}$', '$39_{1}$', '$CV_{20}$']\n",
    "# order_metrics_list = [2, 3, 0, 1, 4]\n",
    "# legend = True\n",
    "# ranking_path = os.path.join(img_path, 'ranking_normalized')\n",
    "# if not os.path.exists(ranking_path):\n",
    "#     os.makedirs(ranking_path)\n",
    "# for analysis in analysis_list:\n",
    "#         df = _df_rank[analysis][_df_rank[analysis]['Scenario'].isin(selected_scenario)]\n",
    "#         sorted_approaches = sort_approaches_by_area(df, metrics_list, order_metrics_list)\n",
    "#         plot_spasa_di_ranking_spider(df,\n",
    "#             r='Value [%]', metric='f1', r_range=(-.1, 1), autoticks=True, plot_std=False, top=None,\n",
    "#             fill=True, show_annotations=True, savefig=os.path.join(ranking_path, 'radar_metric_%s' % analysis),\n",
    "#             show_title=False, axis_angle=90, selected_approaches=selected_approaches_dict[analysis], legend=legend,\n",
    "#                                     sorted_approaches=sorted_approaches, order_metrics=order_metrics_list, dashes=linestyles[analysis],\n",
    "#                                     selected_scenario=selected_scenario, min_max_norm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94aae86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# _df_rank['incr'].loc[(_df_rank['incr']['Approach'] == 'bic') & (_df_rank['incr']['Metric'] == 'F1 Score'), 'Value [%]'] = np.nan\n",
    "# import os\n",
    "\n",
    "# for analysis in selected_approaches_dict:\n",
    "#     ranking_by_area = False\n",
    "#     legend = True\n",
    "    \n",
    "#     ranking_path = os.path.join(img_path, 'ranking%s%s' % ('_by_area' if ranking_by_area else '', '_noleg' if not legend else ''))\n",
    "#     if not os.path.exists(ranking_path):\n",
    "#         os.makedirs(ranking_path)\n",
    "        \n",
    "#     plot_spasa_di_ranking_spider(_df_rank[analysis],\n",
    "#         r='Value [%]', metric='f1', r_range=(0, 100), autoticks=True, plot_std=False, top=None,\n",
    "#         fill=True, show_annotations=True, savefig=os.path.join(ranking_path, 'radar_metric_%s' % analysis),\n",
    "#         show_title=False, axis_angle=90, selected_approaches=selected_approaches_dict[analysis],\n",
    "#                                  ranking_by_area=ranking_by_area, legend=legend, dashes=linestyles[analysis])\n",
    "#     plot_spasa_di_ranking_spider(_df_rank,\n",
    "#         r='Rank', metric='f1', r_range=(len(selected_approaches_dict[analysis]) + 1, 1), autoticks=True, plot_std=False,\n",
    "#         top=None, fill=True, show_annotations=True, savefig=os.path.join(ranking_path, 'radar_rank_%s' % analysis),\n",
    "#         best_mode='min', show_title=False, axis_angle=90, selected_approaches=selected_approaches_dict[analysis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8f155b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# _df_rank['incr'].loc[(_df_rank['incr']['Approach'] == 'bic') & (_df_rank['incr']['Metric'] == 'F1 Score'), 'Value [%]'] = np.nan\n",
    "import os\n",
    "metric = 'F1 Score'\n",
    "discr = ''\n",
    "r = 'Value [%]'\n",
    "\"\"\"metrics_list = ['%s' % metric,\n",
    "        discr + '%s Intransigence' % metric,\n",
    "        discr + '%s Forgetting' % metric,\n",
    "        discr + '%s Drop' % metric,\n",
    "        discr + '%s Drop Old' % metric]\n",
    "\"\"\"\n",
    "metrics_list = ['%s' % metric,\n",
    "        discr + '%s Intransigence' % metric,\n",
    "        discr + '%s Drop' % metric,\n",
    "        discr + '%s Drop Old' % metric]\n",
    "#selected_scenario = ['$38_{2}$', '$39_{1}$', '$CV_{20}$']\n",
    "selected_scenario = ['$39_{1}$', '$CV_{20}$']\n",
    "# ToDo: Generalize order_metrics_list in order to test all area combinations\n",
    "#order_metrics_list = [2, 3, 0, 1, 4]\n",
    "order_metrics_list = [2, 0, 1]\n",
    "#ranking_by_area_list = [False, True]\n",
    "ranking_by_area_list = [False]\n",
    "legend = True\n",
    "sorted_approaches = None\n",
    "analysis_list = ['incr', 'naive']\n",
    "for ranking_by_area in ranking_by_area_list:\n",
    "    if not ranking_by_area:\n",
    "        fig_path = os.path.join(img_path, 'radarplot_final')\n",
    "    else:\n",
    "        fig_path = os.path.join(img_path, 'ranking%s%s' % ('_by_area' if ranking_by_area else '', '_noleg' if not legend else ''))\n",
    "    if not os.path.exists(fig_path):\n",
    "        os.makedirs(fig_path)\n",
    "    for analysis in analysis_list:\n",
    "        df = _df_rank[analysis][_df_rank[analysis]['Scenario'].isin(selected_scenario)]\n",
    "        if ranking_by_area:\n",
    "            print('Sorting approaches by area')\n",
    "            sorted_approaches = sort_approaches_by_area(df, metrics_list, order_metrics_list)\n",
    "            print(sorted_approaches)\n",
    "        else:\n",
    "            sorted_approaches = {scenario: [key for key in appr_dict_alph_sorted if key in selected_approaches_dict[analysis]] for scenario in selected_scenario}\n",
    "        plot_spasa_di_ranking_spider(df,\n",
    "            r='Value [%]', metric='f1', r_range=(95, -5), autoticks=True, plot_std=False, top=None,\n",
    "            fill=True, show_annotations=True, savefig=os.path.join(fig_path, 'radar_metric_%s' % analysis),\n",
    "            show_title=False, axis_angle=90, selected_approaches=selected_approaches_dict[analysis], legend=legend,\n",
    "                                    sorted_approaches=sorted_approaches, order_metrics=order_metrics_list, dashes=linestyles[analysis],\n",
    "                                    selected_scenario=selected_scenario, min_max_norm=False, best_mode='min', fig_format='pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f961b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_spasa_di_ranking_spider(r='Rank', metric='f1', r_range=(len(selected_approaches) + 1, 1), autoticks=True, plot_std=True, top=3, fill=False, show_annotations=True, savefig=os.path.join(ranking_path, 'radar_rank'), best_mode='min', show_title=False, axis_angle=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee657e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _df_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89390fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import math\n",
    "index_list = [i for i in range(len(metrics_list))]\n",
    "perm_list = list(itertools.permutations(index_list))[:int(math.factorial(len(metrics_list)) / len(metrics_list))]\n",
    "are_ranking_equals_per_analysis = {}\n",
    "for analysis in ['incr', 'naive']:\n",
    "    df = _df_rank[analysis][_df_rank[analysis]['Scenario'].isin(selected_scenario)]\n",
    "    ranking_all_perm_list = [sort_approaches_by_area(df, metrics_list, list(perm)) for perm in perm_list]\n",
    "    ranking_per_scenario_dict = {scenario: [] for scenario in selected_scenario}\n",
    "    for ranking_all_scenario in ranking_all_perm_list:\n",
    "        for scenario in selected_scenario:\n",
    "            ranking_per_scenario_dict[scenario].append(ranking_all_scenario[scenario])\n",
    "    are_ranking_equals_per_analysis[analysis] = {}\n",
    "    for scenario in selected_scenario:\n",
    "        ranking_sample = ranking_per_scenario_dict[scenario][0]\n",
    "        are_ranking_equals_per_analysis[analysis][scenario] = all([True if ranking_sample == ranking_per_scenario_dict[scenario][i] else False for i in range(1, len(ranking_per_scenario_dict[scenario]))])\n",
    "            \n",
    "print(are_ranking_equals_per_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6482752",
   "metadata": {},
   "source": [
    "# Approaches Comparison by Metric Value Drop from Previous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9271e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# m=df_rank[(df_rank['Metric'].isin(['F1 Score All', 'F1 Score Drop All'])) & (df_rank['Scenario'] == '$CV_{5}$')].groupby(['Approach', 'Increment', 'Metric', 'Scenario'])[['Rank', 'Value [%]','Drop from Previous [%]']].mean()\n",
    "# m[m['Rank']<3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fa5aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_spasa_di_ranking(y='Drop from Previous [%]', ylim=(None, None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6f78dc",
   "metadata": {},
   "source": [
    "# Approaches Comparison by Metric Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610d87a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_spasa_di_ranking(y='Value [%]', ylim=(None, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe28c62c",
   "metadata": {},
   "source": [
    "# Approaches Comparison by Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac21df5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot_spasa_di_ranking(y='Rank', ylim=(0, 11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837a395c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ac7079",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a328f095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import portalocker as pl\n",
    "# import json\n",
    "\n",
    "# def get_ts(df_filename):\n",
    "#     return '-'.join(df_filename.split('/')[-1].split('-')[1:]).split('_')[0]\n",
    "\n",
    "\n",
    "# def get_args_dict(df_filename):\n",
    "#     ts = get_ts(df_filename)\n",
    "#     try:\n",
    "#         with open('/'.join(df_filename.split('/')[:-2] + ['args-%s.txt' % ts])) as fin:\n",
    "#             return json.loads(fin.read())\n",
    "#     except FileNotFoundError as _:\n",
    "#         with open('/'.join(df_filename.split('/')[:-1] + ['args-%s.txt' % ts])) as fin:\n",
    "#             return json.loads(fin.read())\n",
    "\n",
    "# def preprocess_metrics(metric, metr_dict, mety_dict, df_filenames, plot_path, analysis, override=True, appr_dict=None,\n",
    "#                        nc_incr_tasks=None):\n",
    "#     metric, metric_type = metric\n",
    "#     print('Preprocessing %s %s %s' % (analysis, metric, metric_type))\n",
    "    \n",
    "# #     if metric_type != 'intransigence':\n",
    "# #         return pd.DataFrame()\n",
    "\n",
    "#     material_path = os.path.join(plot_path, 'material')\n",
    "#     with pl.Lock('lock'):\n",
    "#         if not os.path.exists(material_path):\n",
    "#             os.makedirs(material_path)\n",
    "            \n",
    "#     if nc_incr_tasks is None:\n",
    "#         nc_incr_tasks = [0] * len(df_filenames)\n",
    "\n",
    "#     discr = mety_dict[metric_type]\n",
    "#     normal_discr = mety_dict['normal'][analysis]\n",
    "#     try:\n",
    "#         discr = discr[analysis]\n",
    "#     except:\n",
    "#         pass\n",
    "\n",
    "#     average_analysis = analysis in ['all', 'new', 'old']\n",
    "#     normal_metric_col = ' '.join([metr_dict[metric], normal_discr]).strip()\n",
    "\n",
    "#     preprocessed_fn = os.path.join(material_path,\n",
    "#                                    '%s_metrics_%s%s-material.parquet' % (analysis, metric, discr.lower()))\n",
    "#     if os.path.exists(preprocessed_fn) and override is None:\n",
    "#         override = input('File \"%s\" exists: override [Y, n]? ' % preprocessed_fn).lower() != 'n'\n",
    "#     if not os.path.exists(preprocessed_fn) or override:  # Generate or Override\n",
    "#         common_columns = ['Seed', 'Network', 'Approach', 'Episode', 'Batch Size', 'Patience', 'Increment']\n",
    "#         if average_analysis:\n",
    "#             df = pd.DataFrame(columns=common_columns + [normal_metric_col])\n",
    "#         else:  # if analysis == 'per_episode':\n",
    "#             df = pd.DataFrame(columns=common_columns + ['Task', normal_metric_col])\n",
    "#         for df_filename, nc_incr_task in tqdm(list(zip(df_filenames, nc_incr_tasks))):\n",
    "#             args_dict = get_args_dict(df_filename)\n",
    "#             assert args_dict.get('nc_incr_tasks', nc_incr_task) > 0, 'Please set --nc-incr-tasks argument.'\n",
    "#             row = {'Seed': args_dict['seed'], 'Network': args_dict['network'],\n",
    "#                    'Approach': '%s' % appr_dict[args_dict['approach']], 'Batch Size': args_dict['batch_size'],\n",
    "#                    'Patience': args_dict['lr_patience'],\n",
    "#                    'Out Features Size': args_dict.get('out_features_size', 200),\n",
    "#                    'Momentum': args_dict.get('momentum', .0)}\n",
    "#             tmp_df = pd.read_parquet(df_filename)\n",
    "#             tmp_df[metric] = tmp_df[metric].apply(lambda x: [np.nan if v is None else v for v in x])\n",
    "#             if 'nc_first_task' in tmp_df:\n",
    "#                 tmp_df_g = tmp_df.groupby(['nc_first_task', 'nc_incr_tasks'])\n",
    "#             else:\n",
    "#                 tmp_df_g = [((args_dict['nc_first_task'], args_dict['nc_incr_tasks']), tmp_df)]\n",
    "#             for g, tmp_d in tmp_df_g:\n",
    "#                 values = [[v0 * 100 for v0 in v] for v in tmp_d[metric].values]\n",
    "#                 _nc_first_task = g[0]\n",
    "#                 _nc_incr_tasks = g[1]\n",
    "#                 for ep, value in enumerate(values):\n",
    "#                     row.update({'Episode': ep,\n",
    "#                                 'Increment': _nc_incr_tasks,\n",
    "#                                 '#Apps': _nc_first_task + _nc_incr_tasks * ep})\n",
    "#                     if average_analysis:\n",
    "#                         if 'all' in analysis:\n",
    "#                             row.update({normal_metric_col: value[0], 'Type': 'All'})\n",
    "#                             df = df.append(row, ignore_index=True)\n",
    "#                         else:\n",
    "#                             for val, type in zip(value, ['Old', 'New']):\n",
    "#                                 row.update({normal_metric_col: val, 'Type': type})\n",
    "#                                 df = df.append(row, ignore_index=True)\n",
    "#                     else:  # if analysis == 'per_episode':\n",
    "#                         for task, val in enumerate(value):\n",
    "#                             row.update({'Task': task, normal_metric_col: val})\n",
    "#                             df = df.append(row, ignore_index=True)\n",
    "#         assert len(df) > 0\n",
    "#         if metric_type in ['expectation', 'intransigence']:\n",
    "#             df_UB = df[df['Approach'] == 'Scratch'].reset_index(drop=True)\n",
    "#             df = df[df['Approach'] != 'Scratch'].reset_index(drop=True)\n",
    "#             metric_col = ' '.join([metr_dict[metric], discr]).strip()\n",
    "#             columns = [col for col in list(df.columns) if\n",
    "#                        col not in [normal_metric_col, 'Approach', 'Momentum',\n",
    "# #                                    'Episode', 'Increment', 'Task'\n",
    "#                                   ]]\n",
    "#             episode_index = columns.index('Episode')\n",
    "#             try:\n",
    "#                 task_index = columns.index('Task')\n",
    "#             except ValueError as _:\n",
    "#                 pass\n",
    "#             df[metric_col] = np.nan\n",
    "#             df_g = df.groupby(columns)\n",
    "#             groups = list(df_g.groups)\n",
    "#             del df_g\n",
    "#             for group in groups:\n",
    "#                 df_filter = functools.reduce(lambda x, y: x & y, [df[col] == g for col, g in zip(columns, group)])\n",
    "#                 df_UB_filter = functools.reduce(lambda x, y: x & y, [df_UB[col] == g for col, g in zip(columns, group)])\n",
    "#                 if not len(df_UB.loc[df_UB_filter]):\n",
    "#                     print('WARNING: group (%s) does not present Upperbound model.' % ', '.join([str(v) for v in group]))\n",
    "#                     continue\n",
    "#                 else:\n",
    "#                     ub_value = df_UB.loc[df_UB_filter, normal_metric_col].values[0]\n",
    "#                     df.loc[df_filter, metric_col] = df.loc[df_filter, normal_metric_col].apply(\n",
    "#                         (lambda x: x / ub_value * 100) if metric_type == 'expectation' else (lambda x: ub_value - x)\n",
    "#                     )\n",
    "#         df.to_parquet(preprocessed_fn)\n",
    "#     else:\n",
    "#         print('WARNING: using already computed material.')\n",
    "#         df = pd.read_parquet(preprocessed_fn)\n",
    "\n",
    "#     if '#Apps' in df:\n",
    "#         min_classes = df[df['Approach'] != appr_dict['scratch']]['#Apps'].min()\n",
    "#         df = df[df['#Apps'] >= min_classes]\n",
    "\n",
    "#     print(df)\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b65708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# res_path = '/media/nas/datasets/MIRAGE_2020/FSCIL_approaches/hf-project/results'\n",
    "# analysis = 'per_episode_metrics'\n",
    "\n",
    "# ms = ['accuracy_score', 'forgetting_accuracy_score', 'accuracy_score', 'forgetting_f1_score']\n",
    "# m_dict = {'accuracy_score':'Accuracy', 'forgetting_accuracy_score':'Accuracy Forgetting',\n",
    "#           'forgetting_f1_score':'F1 Score Forgetting'}\n",
    "# ts = ['normal', 'normal', 'intransigence', 'normal']\n",
    "# t_dict = {'normal':{analysis:'Per Episode'}, 'intransigence':{analysis:'Per Episode Intransigence'}}\n",
    "# # appr_dict = {'icarl': 'iCaRL', 'icarlo': 'iCaRL-original', 'icarlp': 'iCaRL+', 'bic': 'BiC', 'il2m': 'IL2M',\n",
    "# #                  'lwf': 'LwF', 'finetuning': 'FineTuning', 'lucir': 'LUCIR', 'ewc': 'EWC', 'joint': 'Joint',\n",
    "# #                  'scratch': 'Scratch', 'freezing': 'Fixed-Repr', 'eeil': 'EEIL', 'ssil': 'SS-IL'}\n",
    "\n",
    "# per_episode_dfs = dict()\n",
    "\n",
    "# for scenario_discrs in scenario_discrs_dict:\n",
    "#     per_episode_dfs[scenario_discrs] = dict()\n",
    "#     for img_d in scenario_discrs_dict[scenario_discrs]:\n",
    "#         per_episode_dfs[scenario_discrs][img_d] = dict()\n",
    "#         exp_name = '*'.join([img_d[i:i+2] for i in range(0,len(img_d),2)])\n",
    "#         filenames = glob('%s/*%s/**/*%s.parquet' % (res_path, exp_name, analysis), recursive=True)\n",
    "#         filenames.extend(glob('%s_UB/*%s/**/*%s.parquet' % (res_path, 'scratch', analysis), recursive=True))\n",
    "#         plot_path = glob('%s/%s_*/' % (img_path, img_d))[0]\n",
    "#         for m, t in zip(ms[-1:], ts[-1:]):\n",
    "#             df = preprocess_metrics((m, t), m_dict, t_dict, filenames,\n",
    "#                                     plot_path, analysis, False, appr_dict)\n",
    "#             per_episode_dfs[scenario_discrs][img_d]['%s_%s' % (m, t)] = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3bb031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp_df = per_episode_dfs['CV']['can5ba20inr5stp5']['accuracy_score_normal']\n",
    "# tmp_df[(tmp_df['Episode']==2) & (tmp_df['Task']==2) & (tmp_df['Seed']==0) & (tmp_df['Increment']==5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea62076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp_df = per_episode_dfs['CV']['can5ba20inr5stp5']['forgetting_accuracy_score_normal']\n",
    "# tmp_df[(tmp_df['Episode']==2) & (tmp_df['Task']==2) & (tmp_df['Seed']==0) & (tmp_df['Increment']==5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d3ad6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp_df = per_episode_dfs['CV']['can5ba20inr5stp5']['accuracy_score_intransigence']\n",
    "# tmp_df[(tmp_df['Episode']==2) & (tmp_df['Task']==2) & (tmp_df['Seed']==0) & (tmp_df['Increment']==5)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
